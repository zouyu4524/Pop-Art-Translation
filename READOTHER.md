## è®ºæ–‡ç¿»è¯‘: Learning values across many orders of magnitude

Hado van Hasselt, Arthur Guez, Matteo Hessel, Volodymyr Mnih, David Silver. "Learning values across many orders of magnitude." In *Advances in Neural Information Processing Systems*, pp. 4287-4295. 2016. [[pdf](https://arxiv.org/pdf/1602.07714)]

### ç¬¦å·ç³»ç»Ÿ

| ç¬¦å· 	| å«ä¹‰ | ç¬¦å· | å«ä¹‰ |
| :---: | :--- | :---: | :--- |
| $Y_t$ | ç›®æ ‡ (target) | ${\bf W}$  | æ ‡å‡†åŒ–å±‚æƒé‡ | 
| $\tilde{Y}_t$ | æ ‡å‡†åŒ–åçš„ç›®æ ‡ | ${\boldsymbol b}$ | æ ‡å‡†åŒ–å±‚bias |
| ${\boldsymbol \Sigma}_t$, ${\boldsymbol \mu}_t$ | å­¦ä¹ åˆ°çš„*scale*å’Œ*shift* | $h_{\boldsymbol \theta}$ | æ¨¡å‹å‚æ•° | 
| $g(\cdot)$ | æ ‡å‡†åŒ–å‡½æ•° | $f(\cdot)$ | åŸå‡½æ•°    |
 
### ç¬¬äºŒç«  Adaptive normalization with Pop-Art

> We propose to normalize the targets $Y_t$, where the normalization is learned **separately** from the approximating function. We consider an affine transformation of the targets

$$
\tilde{Y}_ t = {\boldsymbol \Sigma}_ t^{-1} (Y_ t - {\boldsymbol \mu}_ t),
$$

æœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å­¦ä¹ æ ‡å‡†åŒ–è¿‡ç¨‹ä¸å‡½æ•°æ‹Ÿåˆä¸¤ä¸ªè¿‡ç¨‹åˆ†ç¦»å¼€, å…·ä½“åˆ†ç¦»çš„æ–¹å¼æ˜¯åœ¨åŸæœ‰çš„å‡½æ•°æ‹Ÿåˆç½‘ç»œä¸‹æ¥ä¸€å±‚**affine transformation**(çº¿æ€§å˜æ¢)ã€‚åŸæœ‰çš„å‡½æ•°è´Ÿè´£å­¦ä¹ å‡½æ•°æ‹Ÿåˆ, è€Œä¸‹æ¥çš„ä¸€å±‚çº¿æ€§å˜æ¢å±‚è´Ÿè´£"è·Ÿè¸ª"|å­¦ä¹ æ ‡å‡†åŒ–çš„å‚æ•°ã€‚

> We can then define a loss on a normalized function $g(X_t)$ and the normalized target $\tilde{Y}_ T$. THe unnormalized approximation for any input $x$ is then given by $f(x) = {\boldsymbol \Sigma} g(x) + {\boldsymbol \mu}$, where $g$ is the *normalized function* and $f$ is the *unnormalized function*.

> Thereby, we decompose the problem of learning an appropriate normalization from learning the specific shape of the function. The two properties that we want to simultaneosly achieve are

> (**ART**) to update scale ${\boldsymbol \Sigma}$ and shift ${\boldsymbol \mu}$ such that ${\boldsymbol \Sigma}^{-1} (Y - {\boldsymbol \mu})$ is approproately normalized, and  
> (**POP**) to preserve the outputs of the unnormalized function when we change the scale and shift.

å…¶ä¸­**ART**è´Ÿè´£å­¦ä¹ æ ‡å‡†åŒ–å‚æ•°, è€Œ**POP**è´Ÿè´£ä¿éšœä»»ä½•çš„æ ‡å‡†åŒ–å‚æ•°ä¸‹æ€»èƒ½ä»æ ‡å‡†åŒ–åçš„ç»“æœå¤åŸåŸå§‹çš„è¾“å‡ºã€‚æœ‰äº†ä¸¤ç‚¹çš„ç»“åˆå°±èƒ½å®ç°é’ˆå¯¹ä»»æ„è¾“å‡ºæƒ…å†µä¸‹ç”¨åˆé€‚çš„æ ‡å‡†åŒ–å‚æ•°è¿›è¡Œnormalization, ä¸æ­¤åŒæ—¶ä¿éšœä¸ç ´åå·²ç»å­¦ä¹ åˆ°å‡½æ•°æ‹Ÿåˆç»“æœã€‚

#### 2.1 POP: Preserving outputs precisely

> Unless care is taken, repeated updates to the normalization might make learning harder rather than easier because the normalized targets become non-stationary. More importantly, whenever we adapt the normalization based on a certain target, this would simultaneously change the output of the unnormalized function of all inputs.

æœ¬æ®µè§£é‡Šäº†æå‡º**POP**çš„motivationã€‚å¯æƒ³è€ŒçŸ¥, å¦‚æœæ— æ³•ä¿éšœåŸå§‹ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰çš„targetä¸èƒ½è¢«å¤åŸçš„è¯, é‚£ä¹ˆä¸€æ—¦åœ¨å­¦ä¹ æ—¶é‡‡ç”¨äº†ä¸åŒçš„æ ‡å‡†åŒ–å‚æ•°, åŸæœ‰å­¦ä¹ åˆ°çš„æ¨¡å‹å°†å—åˆ°å½±å“ï¼ˆç ´åï¼‰ã€‚ä¸¾ä¾‹è€Œè¨€, å‡è®¾é¦–å…ˆå­¦ä¹ åˆ°çš„æ¨¡å‹æ˜¯é€šè¿‡åŸå§‹targetèŒƒå›´åœ¨$[0, 1]$å†…, è€Œåå‡ºç°çš„æ ·æœ¬targetèŒƒå›´åœ¨$[10, 100]$, å°†å…¶æ ‡å‡†åŒ–è‡³$[0, 1]$å†…åä½œä¸ºæ–°çš„æ ·æœ¬è®­ç»ƒå·²æœ‰æ¨¡å‹, é‚£ä¹ˆåŸæ¥æ¨¡å‹çš„æ‰€ç»™å‡ºçš„$[0, 1]$èŒƒå›´ç»“æœå°†å—åˆ°"æ³¢åŠ"ã€‚

> The **only way** to avoid changing all outputs of the unnormalized function whenever we update the scale and shift is by changing the normalized function $g$ itself simultaneously. The goal is the preserve the outputs from before the change of normalization, for all inputs. This prevents the normalization from affecting the approximation, which is appropriate because its objective is solely to make learning easier, and to leave solving the approximation itself to the optimization algorithm.

è§£å†³æ­¤é—®é¢˜çš„å”¯ä¸€åŠæ³•å°±æ˜¯åœ¨æ›´æ–°æ ‡å‡†åŒ–å‚æ•°çš„åŒæ—¶æ›´æ–°åŸæœ‰æ ‡å‡†åŒ–å‰çš„è¾“å‡ºã€‚æ ‡å‡†åŒ–çš„ç›®çš„åœ¨äºä½¿å¾—è®­ç»ƒé›†åœ¨å·®ä¸å¤šçš„è§„æ¨¡å†…ä»è€Œä½¿æ¨¡å‹è®­ç»ƒæ›´ä¸ºå®¹æ˜“, éœ€è¦é˜²æ­¢æ ‡å‡†åŒ–å½±å“æ¨¡å‹è®­ç»ƒæœ¬èº«ã€‚

> Without loss of generality the unnormalized function can be written as  
$$
f_ { {\boldsymbol \theta, \Sigma}, {\bf W}, {\boldsymbol b} } (x) \equiv {\boldsymbol \Sigma} g_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (x) + {\boldsymbol \mu} \equiv {\boldsymbol \Sigma} ( {\bf W} h_ { {\boldsymbol \theta} } (x) + {\boldsymbol b}) + {\boldsymbol \mu},
$$ 

å…¶ä¸­$g_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (x) = {\bf W} h_ { {\boldsymbol \theta} } (x) + {\boldsymbol b}$ ä¸ºæ ‡å‡†åŒ–å‡½æ•°, è€Œ$h_ { {\boldsymbol \theta} } (x)$ æ˜¯å‡½æ•°æ‹Ÿåˆçš„ç½‘ç»œ(non-linear)ã€‚

> It is not uncommon for deep neural networks to end in a linear layer, and the $h_ { {\boldsymbol \theta} }$ can be the output of the last (hidden) layer of non-linearities. Alternatively, we can always add a square linear layer to any non-linear function $h_ { {\boldsymbol \theta} }$ to ensure this constraint, for instance initialized as ${\bf W}_ 0 = {\bf I}$ and ${\boldsymbol b}_ 0 = {\boldsymbol 0}$.

æœ¬æ®µè§£é‡Šäº†ç½‘ç»œæ¶æ„, ä¸€èˆ¬è€Œè¨€DNNçš„æœ€åä¸€å±‚ï¼ˆè¾“å‡ºå±‚ï¼‰å‡å…·å¤‡çº¿æ€§æ¿€æ´», å³æ»¡è¶³ä»¥ä¸Šçš„æ¡ä»¶; è‹¥ä¸ç„¶, åˆ™å¯ä»¥åœ¨åŸæœ‰ç½‘ç»œçš„åŸºç¡€ä¸ŠåŠ ä¸Šä¸€ä¸ªçº¿æ€§å±‚(æœ¬å±‚å‚æ•°è§„æ¨¡ä¸º$k\times k | k$, $k$ä¸ºè¾“å‡ºçš„å±‚çš„ç¥ç»å…ƒæ•°é‡), ä¸æ”¹å˜ç½‘ç»œæ¶æ„çš„åŒæ—¶æ»¡è¶³äº†ä»¥ä¸Šçš„æ¡ä»¶ã€‚

> **Proposition 1.** *Consider a function $f: \mathcal{R}^n \rightarrow \mathcal{R}^k$ as*
$$
f_ { {\boldsymbol \theta, \Sigma}, {\bf W}, {\boldsymbol b} } (x) \equiv {\boldsymbol \Sigma} ( {\bf W} h_ { {\boldsymbol \theta} } (x) + {\boldsymbol b}) + {\boldsymbol \mu},
$$  

> *where $h_ {\boldsymbol \theta}: \mathcal{R}^n \rightarrow \mathcal{R}^m$ is any non-linear function of $x\in \mathcal{R}^n$, ${\boldsymbol \Sigma}$ is a $k\times k$ matrix, ${\boldsymbol \mu}$ and ${\boldsymbol b}$ are $k$-element vectors, and ${\bf W}$ is a $k\times m$ matrix. Consider any change of the scale and shift parameters from ${\boldsymbol \Sigma}$ to ${\boldsymbol \Sigma}_ \text{new}$ and from ${\boldsymbol \mu}$ to ${\boldsymbol \mu}_ \text{new}$, where ${\boldsymbol \Sigma}_ \text{new}$ is non-singular. If we then additionally change the parameters ${\bf W}$ and ${\boldsymbol b}$ to ${\bf W}_ \text{new}$ and ${\boldsymbol b}_ \text{new}$, defined by*  

$$ 
{\bf W}_ \text{new} = {\boldsymbol \Sigma}_ \text{new}^{-1} {\boldsymbol \Sigma} {\bf W} \quad \text{and} \quad {\boldsymbol b}_ \text{new} = {\boldsymbol \Sigma}_ \text{new}^{-1} \left( {\boldsymbol \Sigma b + \mu - \mu}_ \text{new} \right)
$$  

> *then the outputs of the unnormalized function $f$ are preserved precisely in the sense that*  

$$
f_ { {\boldsymbol \theta, \Sigma}, {\bf W}, {\boldsymbol b} } (x) = f_ { {\boldsymbol \theta, \Sigma}_ \text{new}, {\bf W}_ \text{new}, {\boldsymbol b}_ \text{new} } (x), \quad \forall x.
$$

ä»¥ä¸Šçš„Propositionè¡¨æ˜åªè¦é‡‡ç”¨**é€‚å½“çš„**æ ‡å‡†åŒ–å‚æ•°å˜æ¢æ–¹æ¡ˆæ€»èƒ½ä¿è¯åŸå‡½æ•°æ‹Ÿåˆçš„ç»“æœä¿æŒä¸å˜ã€‚å…¶ä¸­é€‚å½“çš„å˜åŒ–æ–¹æ¡ˆå³ä»¥ä¸Šç»™å‡ºçš„${\bf W}_ \text{new}$ä¸${\boldsymbol b}_ \text{new}$çš„æ›´æ–°æ–¹æ¡ˆã€‚

> For the special case of scalar scale and shift, with ${\boldsymbol \Sigma} \equiv \sigma {\bf I}$ and ${\boldsymbol \mu} \equiv \mu {\boldsymbol 1}$, the updates to ${\bf W}$ and ${\boldsymbol b}$ become ${\bf W}_ \text{new} = (\sigma/\sigma_\text{new}) {\bf W}$ and ${\boldsymbol b}_ \text{new} = (\sigma {\boldsymbol b} + {\boldsymbol \mu} - {\boldsymbol \mu}_ \text{new})/\sigma_ \text{new}$. After updating the scale and shift we can update the output of the normalized function $g_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (X_ t)$ toward the normalized $\tilde{Y}_ t$, using any learning algorithm.

ä»¥ä¸Šç»™å‡ºäº†ç‰¹ä¾‹, å³å½“scaleå’Œshiftå‡ä¸ºæ ‡é‡ã€‚æŒ‰ä»¥ä¸Šè§„åˆ™æ›´æ–°äº†æ ‡å‡†åŒ–å‚æ•°å, æ ¹æ®**Proposition 1**å¯çŸ¥åŸæ¥çš„è¾“å‡ºåœ¨æ–°çš„æ ‡å‡†åŒ–å‚æ•°ä¸‹å¹¶ä¸ä¼šæ”¹å˜, è€Œæ–°çš„è®­ç»ƒæ•°æ®é€šè¿‡æ–°çš„æ ‡å‡†åŒ–å‚æ•°å¤„ç†åå³å¯ç”¨äºå¯¹å‡½æ•°æ‹Ÿåˆæ¨¡å‹(å³$h_ { {\boldsymbol \theta} }$)çš„è®­ç»ƒã€‚

<p align="center">
<img src="https://user-images.githubusercontent.com/16682999/64672136-33788080-d49d-11e9-9771-bc07a48e99b6.png" alt="algorithm 1" width="800">
</p>

> Algorithm 1 is an example implementation of SGD with Pop-Art for a squared loss. It can be generalized easily to any other loss by changin the definition of ${\boldsymbol \delta}$. Notice that ${\bf W}$ and ${\boldsymbol b}$ are updated twice: first to adapt to the new scale and shift to preserve the outputs of the function, and then by SGD. The order of these updates is important because it allows us to use the new normalization immediately in the subsequent SGD update.

åœ¨æ­¤åŸºç¡€ä¸Š, ç®—æ³•1ä»¥MSEä¸ºlosså‡½æ•°, SGDä¸ºoptimizerä¸ºä¾‹é˜è¿°äº†å¦‚ä½•å®ç°Pop-Artç®—æ³•ã€‚å…¶ä¸­ä¸»è¦åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µ:  

- æ›´æ–°æ ‡å‡†åŒ–å‚æ•°(Pop)  
- æ›´æ–°å‡½æ•°æ‹Ÿåˆæ¨¡å‹å†…å±‚($h_ { {\boldsymbol \theta} }$), ç®—æ³•ä¸­çº¢æ¡†æ ‡æ³¨éƒ¨åˆ†  
- ç”±SDGæ›´æ–°æ ‡å‡†åŒ–å±‚å‚æ•°

**å°ç»“**: æœ¬èŠ‚ä¸»è¦é˜è¿°äº†Pop-Artç®—æ³•ä¸­çš„Popéƒ¨åˆ†, å³å¦‚ä½•æ›´æ–°æ ‡å‡†åŒ–å‚æ•°ä»¥ä¿è¯åç»­çš„è®­ç»ƒå¯ä»¥ä¿éšœä¸å½±å“å·²æœ‰è¾“å‡ºçš„ç»“æœã€‚å¦ä¸€æ–¹é¢, é€šè¿‡ç®—æ³•1ç»™å‡ºäº†ç»“åˆPop-Artçš„ç½‘ç»œæ›´æ–°æµç¨‹, å…¶ä¸­æ ‡å‡†åŒ–å±‚çš„å‚æ•°å°†è¢«æ›´æ–°ä¸¤æ¬¡, ç¬¬ä¸€æ¬¡ä¸ºä¿éšœPop, ç¬¬äºŒæ¬¡ä¸ºä¼˜åŒ–ç®—æ³•ä¸‹çš„å‚æ•°æ›´æ–°ã€‚

#### 2.2 ART: Adaptively rescaling targets

å‰ä¸€èŠ‚ä¸­ä»‹ç»äº†æ ‡å‡†åŒ–å‚æ•°æ›´æ–°è¿‡ç¨‹ä¸­ä¿éšœå·²æœ‰è¾“å‡ºä¸å˜çš„åŸºæœ¬æ€è·¯, è€Œæœ¬èŠ‚å°†å…·ä½“ç»™å‡ºå¦‚ä½•è¿›è¡Œæ ‡å‡†åŒ–, å³Art: Adaptively rescaling targetsçš„å†…æ¶µã€‚

> A natual choice is to normalize the targets to approximately have zero mean and unit variance. For clarity and conciseness, we consider scalar normalizations. If we have data $\{ ( X_ i, Y_ i ) \}_ {i=1}^t$ up to some time $t$, we then may desire

$$
\begin{aligned}
& \sum_ {i=1}^t (Y_ i - \mu_ t) / \sigma_ t = 0 \quad & \text{and} \quad & \frac{1}{t} \sum_ {i=1}^t ( Y_ i - \mu_ t )^2 / \sigma_t ^2 = 1,\\
\text{such that} \quad \mu_t &=\frac{1}{t}\sum_ {i=1}^t Y_i \quad & \text{and} \quad \sigma_t^2 &=\frac{1}{t} \sum_ {i=1}^t Y_i^2 - \mu_t^2.
\end{aligned}
$$

> This can be generalized to incremental updates

$$
\mu_t = (1-\beta_t) \mu_{t-1} +\beta_t Y_t~\text{and}~\sigma_t^2 = \nu_t - \mu_t^2, \text{where}~ \nu_t = (1 - \beta_t) \nu_{t-1} + \beta_t Y_t^2.
$$

> Here $\nu_t$ estimates the second moment of the targets and $\beta_t\in [0, 1]$ is a step size. If $\nu_t -\mu_t^2$ is positive initially then it will always remain so, although to avoid issues with numerical precision it can be useful to enforce a lower bound explicitly by requiring $\nu_t -\mu_t^2 \geq \epsilon$ with $\epsilon >0$. For full equivalence to above one we can use $\beta_t = 1/t$. If $\beta_t = \beta$ is constant we get exponential moving averages, placing more weight on recent data points which is appropriate in non-stationary settings.

è‡ªç„¶åœ°, æ ‡å‡†åŒ–çš„æ€è·¯æ˜¯å°†ç°æœ‰çš„æ•°æ®ç»Ÿä¸€åˆ°å‡å€¼ä¸º0, æ–¹å·®ä¸º1çš„è§„æ¨¡, è€ƒè™‘åˆ°æ•°æ®æœ¬èº«æ˜¯æºæºä¸æ–­å‡ºç°çš„, æ›´ä¸€èˆ¬åœ°å¯ä»¥ç”¨ä»¥ä¸Šçš„å¢é‡å¼æ›´æ–°æ–¹å¼updateå‡å€¼($\mu_t$)å’Œæ ‡å‡†å·®($\sigma_t$)ã€‚ä¸¤ç§ç‰¹æ®Šæƒ…å†µ: 1) $\beta_t = 1/t$, åˆ™æ¯ä¸ªæ ·æœ¬çš„æƒé‡ä¸€è‡´; 2) $\beta_t=\beta$ä¸ºå®šå€¼, åˆ™æ„å‘³ç€æŒ‡æ•°å¼çš„æ»‘åŠ¨å¹³å‡, æœ€è¿‘çš„æ ·æœ¬å…·æœ‰æ›´é«˜çš„æƒé‡, é€‚ç”¨äºnon-stationaryçš„æƒ…å†µã€‚å¦ä¸€æ–¹é¢, ä»æ•°å€¼ç²¾åº¦è€ƒè™‘, æœ‰å¿…è¦ä¸ºæ›´æ–°çš„$\sigma_t^2$(å³$\nu_t - \mu_t^2$)è®¾ç½®ä¸€ä¸ªä¸‹é™, $\epsilon$, ä»¥é˜²å‡ºç°é™¤0çš„æƒ…å†µã€‚

> A constant $\beta$ has the additional benefit of never becoming negligibly small. Consider the first time a target is observed that is much larger than all previously observed targets. If $\beta_t$ is small, our statistics would adapt only slightly, and the resulting update may be large enough to harm the learning. If $\beta_t$ is not too small, the normalization can adapt to the large target before updating, potentially making learning more robust.

æœ¬æ®µè¿›ä¸€æ­¥è§£é‡Šäº†$\beta$å–å®šå€¼çš„ä¸€ä¸ªä¼˜ç‚¹ã€‚å‡è®¾$\beta$é€æ­¥é€’å‡è‡³è¾ƒå°å€¼æ—¶, è‹¥æ­¤æ—¶é¦–æ¬¡å‡ºç°ä¸€ä¸ªç›¸å¯¹å¤§çš„ç›®æ ‡å€¼, ç”±äº$\beta$å¤ªå°, å¯¼è‡´æ–°çº³å…¥çš„æ ·æœ¬å¯¹å·²æœ‰çš„ç»Ÿè®¡é‡($\mu$å’Œ$\sigma$)çš„å½±å“å¾ˆå¾®å°, å¯ä»¥è¿‘ä¼¼è®¤ä¸ºç»Ÿè®¡é‡ä¸å˜, é‚£ä¹ˆæ ¹æ®ç®—æ³•1, å…¶ä¸­å…³é”®æ­¥éª¤1å¯¹ç»Ÿè®¡é‡çš„æ›´æ–°å°±å¯ä»¥å¿½ç•¥, æ­¤æ—¶ç¬¬äºŒæ­¥ä¸­ç”±äºæ–°çš„æ ·æœ¬targetå€¼è¿‡å¤§å¯¼è‡´å¯¹æ¨¡å‹çš„è®­ç»ƒäº§ç”Ÿå¤§çš„å½±å“ã€‚ç›¸å, å¦‚æœ$\beta$ä¸å¤ªå°, è¿™æ ·çš„æƒ…å†µä¸‹, åœ¨å‡½æ•°æ‹Ÿåˆç½‘ç»œ($h_ { {\boldsymbol \theta} }$)ä¹‹å‰, è¾ƒå¤§çš„targetå€¼å°†ç”±ç»Ÿè®¡é‡çš„æ›´æ–°è€Œå‰Šå¼±è¿›è€Œå¢å¼ºæ¨¡å‹æ•´ä½“çš„é²æ£’æ€§ã€‚

> **Proposition 2.** *When using updates above to adapt the normalization parameters $\sigma$ and $\mu$, the normalized targets are bounded for all $t$ by*
$$
-\sqrt{(1 - \beta_t) / \beta_t} \leq (Y_t - \mu_t) / \sigma_t \leq \sqrt{(1 - \beta_t) / \beta_t}.
$$

> For instance, if $\beta_t = \beta = 10^{-4}$ for all $t$, then the normalized target is guaranteed to be in $(-100, 100)$. 

ä»¥ä¸Šçš„Propositionè¡¨æ˜äº†ä¸Šè¿°å¢é‡å¼ç»Ÿè®¡å‚æ•°æ›´æ–°ä¸‹è·å¾—çš„targetèŒƒå›´ä¸$\beta_t$çš„å…³ç³», å¯¹äºå‚æ•°é€‰æ‹©å…·æœ‰ä¸€å®šæŒ‡å¯¼æ•ˆæœã€‚

> It is an open question whether it is uniformly best to normalize by mean and variance. In the appendix we discuss other normalization updates, based on percentiles and mini-batches, and derive correspondences between all of these.

ä»¥ä¸Šç»™å‡ºçš„æ˜¯å‡å€¼|æ ‡å‡†å·®çš„æ ‡å‡†åŒ–æ–¹å¼, è¯¥ç§æ–¹å¼æ˜¯å¦å…·æœ‰æ™®é€‚æ€§ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾å¼çš„é—®é¢˜ã€‚ä½œè€…åœ¨é™„å½•ä¸­è®¨è®ºäº†å…¶ä»–çš„æ ‡å‡†åŒ–æ–¹å¼, å°¤å…¶æ˜¯**mini-batches**å€¼å¾—å…³æ³¨ã€‚

**å°ç»“**: æœ¬èŠ‚ä»‹ç»äº†Pop-Artä¸­çš„"Art"éƒ¨åˆ†, å³è‡ªé€‚åº”targetç¼©æ”¾ã€‚æå‡ºäº†ä¸€ä¸ªå¢é‡å¼çš„æ ‡å‡†åŒ–æ–¹å¼, å…¶ä¸­é€šè¿‡å‚æ•°$\beta_t$çš„ä¸åŒå–å€¼æ–¹å¼å¯ä»¥åº”å¯¹ä¸åŒçš„åœºæ™¯ã€‚é’ˆå¯¹target non-stationaryçš„æƒ…å½¢, å»ºè®®ä½¿ç”¨$\beta_t$ä¸ºå®šå€¼, ä»¥å¢å¼ºæ¨¡å‹é²æ£’æ€§, ä¾¿äºåº”å¯¹targetå‡ºç°çªç„¶çš„å˜åŒ–ã€‚ï¼ˆå¦åˆ™, è‹¥$\beta_t$éš$t$å‡å°, "å¼‚å¸¸"çš„targetå¯¹ç»Ÿè®¡å‚æ•°çš„å½±å“å°†å¾ˆå°, ä½†å¯¹æ¨¡å‹æ›´æ–°çš„"ä¼¤å®³"å´å¾ˆå¤§ã€‚ï¼‰

#### 2.3 An equivalence for stochastic gradient descent

> We now step back and analyze the effect of the magnitude of the errors on the gradients when using regular SDG. This analysis suggests a different normalization algorithm, which has an interesting correspondence to Pop-Art SGD.

åœ¨æœ¬èŠ‚ä¸­, ä½œè€…å°†è®¨è®ºè·¨åº¦å¹¿æ³›çš„targetå¯¹SGDç®—æ³•çš„å½±å“, å¹¶æå‡ºç›¸åº”çš„æ ‡å‡†åŒ–ç®—æ³•ã€‚è¯¥ç®—æ³•ä¸å‰è¿°çš„Pop-Art SGD (ç®—æ³•1)æœ‰ç€æœ‰è¶£çš„å…³è”ã€‚

> We consider SGD updates for an unnormalized multi-layer function of form $f_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (X) = {\bf W} h_ { \boldsymbol \theta}(X) + {\boldsymbol b}$. The update for the weight matrix ${\bf W}$ is 

$$
{\bf W}_ t = {\bf W}_ {t-1} + \alpha_t {\boldsymbol \delta}_ t h_ { { \boldsymbol \theta}_ t} (X_t)^\intercal,
$$

> where ${\boldsymbol \sigma}_ t = f_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (X) - Y_t$ is gradient of the squared loss, which we here call **unnormalized error**. The magnitude of this update depends linearly on the magnitude of the error, which is appropriate when the inputs are normalized, because then the ideal scale of the weights depends linearly on the magnitude of the targets.

æœ¬æ®µç€æ‰‹åˆ†ææœ€åä¸€å±‚çš„æƒé‡(${\bf W}$)çš„æ›´æ–°ä¸è¯¯å·®é¡¹($f_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (X) - Y_t$)ä¹‹é—´çš„å…³ç³», æŒ‡å‡ºæƒé‡çš„æ›´æ–°è§„æ¨¡ä¸è¯¯å·®é¡¹çš„è§„æ¨¡å‘ˆçº¿æ€§å…³ç³»ã€‚è¿›ä¸€æ­¥è¯¯å·®é¡¹æœ¬èº«çš„è§„æ¨¡åˆä¸targetçš„è§„æ¨¡å‘ˆçº¿æ€§å…³ç³»ã€‚æ­£å¸¸æƒ…å†µä¸‹(å³targetçš„è§„æ¨¡åœ¨ç›¸å¯¹å°çš„èŒƒå›´å†…æ—¶), ä»¥ä¸Šçš„æ›´æ–°æ˜¯æ²¡é—®é¢˜çš„ã€‚

> Now consider the SGD update to the parameters of $h_ { {\boldsymbol \theta} }$, ${\boldsymbol \theta}_ t = {\boldsymbol \theta}_ {t-1}  - \alpha {\boldsymbol J}_ t {\bf W}_ {t-1}^\intercal {\boldsymbol \delta}_ t$ where ${\boldsymbol J}_ t = ( \nabla h_ { {\boldsymbol \theta}, 1 }  (X), \ldots, \nabla h_ { {\boldsymbol \theta}, m } (X) )^\intercal$ is the Jacobian for $h_ { {\boldsymbol \theta} }$. The magnitudes of both the weights ${\bf W}$ and the erros ${\boldsymbol \delta}$ depend linearly on the magnitude of the targets. This means that the magnitude of the update for ${\boldsymbol \theta}$ depends **quaratically** on the magnitude of the targets. *There is no compelling reason for these updates to depend at all on these magnitudes because the weights in the top layer already ensure appropriate scaling.* In other words, for each doubling of the magnitudes of the targets, the updates to the lower layers quadruple for no clear reason.

è¿›ä¸€æ­¥, lower layerçš„ç½‘ç»œå‚æ•°(${\boldsymbol \theta}$)æ›´æ–°å¹…åº¦æ ¹æ®å…¬å¼æ—¢ä¸${\bf W}$è§„æ¨¡å‘ˆçº¿æ€§å…³ç³», åˆä¸è¯¯å·®é¡¹è§„æ¨¡å‘ˆçº¿æ€§å…³ç³», ä¸¤è€…å‡ä¸targetçš„è§„æ¨¡å‘ˆçº¿æ€§å…³ç³»å¹¶ä¸”æ˜¯ç›¸ä¹˜çš„å…³ç³», ç»¼åˆå¯¼è‡´lower layerç½‘ç»œå‚æ•°æ›´æ–°å¹…åº¦ä¸targetçš„è§„æ¨¡å‘ˆå¹³æ–¹å…³ç³»ã€‚è€Œäº‹å®ä¸Šå¹¶æ²¡æœ‰æ˜¾è‘—çš„ç†ç”±ä¿è¯å¦‚æ­¤çš„å…³ç³»ã€‚ï¼ˆç›¸åè¿™å¯èƒ½é€ æˆå¯¹å‚æ•°çš„"ç ´å"ï¼‰

**æ³¨**: ä¸€èˆ¬å¤šå±‚ç½‘ç»œä¸­, lower layer | upper layerçš„æ¦‚å¿µæ˜¯æ ¹æ®æ­å»ºçš„é¡ºåºè€Œè¨€, æ¥è¿‘Inputçš„ä¸ºlower layer, è€Œæ¥è¿‘Outputçš„ä¸ºupper layerã€‚ï¼ˆæ­¤å‰å¯¹è¿™ä¸¤è€…ç†è§£æ­£å¥½ç›¸åğŸ¤£ï¼‰

> This analysis suggests an algorithmic solution, which seems to be novel in and of itself, in which we track the magnitude of the targets in a separate parameter $\sigma_t$, and then multiply the updates for all lower layers with a factor $\sigma_t^{-2}$. A more general version of this for matrix scallings is given in Algorithm 2.

æ ¹æ®ä»¥ä¸Šçš„åˆ†æ, ä½œè€…æå‡ºäº†ç›¸åº”çš„è§£å†³æ–¹æ¡ˆ, å³è·Ÿè¸ªtargetçš„è§„æ¨¡$\sigma_t$, å¹¶å¯¹lower layerçš„æ›´æ–°ç›¸åº”ä¹˜ä»¥$\sigma_t^{-2}$ä»è€Œæ¶ˆé™¤æ­¤å¤„å¼•å…¥çš„targetè§„æ¨¡çš„å½±å“ã€‚å…·ä½“çš„ç®—æ³•æµç¨‹åœ¨Algorithm 2ä¸­ç»™å‡ºï¼ˆå…¶ä¸­${\bf W} \leftarrow {\bf W} - \alpha {\boldsymbol \delta} {\boldsymbol g}^\intercal$ä¸­${\boldsymbol g}$åº”è¯¥æ˜¯${\boldsymbol h}$ï¼‰ã€‚

<p align="center">
<img src="https://user-images.githubusercontent.com/16682999/64753079-747f9c00-d554-11e9-9bb3-e93407d09307.png" alt="algorithm 2" width="800">
</p>

ç®—æ³•2ä¸­çš„æ ¸å¿ƒæ­¥éª¤æ˜¯${\boldsymbol \theta} \leftarrow {\boldsymbol \theta} - \alpha {\boldsymbol J} ( {\color{red} {\boldsymbol \Sigma} ^{-1} } {\bf W})^\intercal {\color{red} {\boldsymbol \Sigma}^{-1} } {\boldsymbol \delta}$, å…¶ä¸­å¯¹${\bf W}$å’Œ${\boldsymbol \delta}$åˆ†åˆ«åšäº†"åŠæ ‡å‡†åŒ–"å¤„ç†, å³å°†å…¶magnitudeç½®ä¸ºä¸€ã€‚

**å°ç»“**: è‡³æ­¤, ä½œè€…æå‡ºäº†ç¬¬äºŒä¸ªç®—æ³•ã€‚è¯¥ç®—æ³•çš„æ€è·¯ä¸åŒäºç®—æ³•1å¯¹targetè¿›è¡Œæ ‡å‡†åŒ–, è€Œæ˜¯çœ‹å‡†targetè§„æ¨¡å¯¹æ¨¡å‹å‚æ•°updateçš„å½±å“ä¼šåˆ†åˆ«é€šè¿‡è¯¯å·®é¡¹(${\boldsymbol \delta}_ t$)å’Œtop layeræƒé‡(${\bf W}$)å¼•å…¥è€Œé€ æˆquadraticçš„å½±å“, è€ƒè™‘åœ¨lower layeræ¨¡å‹å‚æ•°updateä¸­æ¶ˆé™¤è¯¥å½±å“ã€‚

> We prove an interesting, and perhaps surprising, connection to the Pop-Art algorithm.

> **Proposition 3.** *Consider two functions defined by*

$$
f_ { {\boldsymbol \theta}, {\boldsymbol \Sigma}, {\bf W}, {\boldsymbol b} } (x) = {\boldsymbol \Sigma}({\bf W} h_ { {\boldsymbol \theta} } (x) + {\boldsymbol b} ) + {\boldsymbol \mu} \quad \text{and} \quad f_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (x) = {\bf W} h_ { {\boldsymbol \theta} } (x) + {\boldsymbol b},
$$

> *where $h_ { {\boldsymbol \theta} }$ is the same differentiable function in both cases, and the functions are initialized identically, using ${\boldsymbol \Sigma}_ 0 = {\bf I}$ and ${\boldsymbol \mu}={\bf 0}$, and the same initial ${\boldsymbol \theta}_ 0$, ${\bf W}_ 0$ and ${\boldsymbol b}_ 0$. Consider updating the first function using Algorithm 1 (Pop-Art SGD) and the second using Algorithm 2 (Normalized SDG). Then, for any sequence of non-singular scales $\{ {\boldsymbol \Sigma}_ t \}_ {t=1}^\infty$ and shift $\{ {\boldsymbol \mu}_ t \}_ {t=1}^\infty$, the algorithms are equivalent in the sense that 1) the sequences $\{ {\boldsymbol \theta}_ t \}_ {t=0}^\infty$ are identical, 2) the outputs of the functions are identical, for any input.*

ä»¥ä¸Šçš„Propositioné˜è¿°äº†ç®—æ³•2å’Œç®—æ³•1çš„ç­‰æ•ˆæ€§ã€‚å½“åˆå§‹çš„ä¸Šä¸‹å±‚æƒé‡ä¸€è‡´æ—¶, å¹¶ä¸”ç®—æ³•1ä¸­åˆå§‹çš„ç»Ÿè®¡ç®—æ³•æŒ‰ç…§"å•ä½åŒ–"è®¾ç½®, åˆ™ä¸¤ä¸ªç®—æ³•ä¸‹åœ¨ä»»æ„æ—¶åˆ»çš„ç»“æœå‡æ˜¯ä¸€è‡´çš„ã€‚

> **The proposition shows a duality between normalizing the targets, as in Algorithm 1, and changing the updates, as in Algorithm 2.** This allows us to gain more intuition about the algorithm. In particular, in Algorithm 2 the updates in top layer are not normalized, thereby allowing the last linear layer to adapt to the scale of the targets. That said, these methods are complementary, and it is straightforward to combing Pop-Art with other optimization algorithms than SGD.

å†æ¬¡å¼ºè°ƒä»¥ä¸ŠPropositionçš„é‡è¦æ€§, ä¿éšœäº†ä¸¤ä¸ªç®—æ³•é—´çš„å¯¹å¶æ€§ã€‚å¦ä¸€æ–¹é¢, ç®—æ³•2ä¸­çš„top layerå¹¶æœªè¿›è¡Œæ ‡å‡†åŒ–å¤„ç†, ä»è€Œä¿ç•™äº†å…¶é€‚åº”targetçš„è°ƒæ•´ç©ºé—´ã€‚Propositionè¡¨æ˜Pop-Artç®—æ³•æœ¬èº«å¯ä»¥ä½œä¸ºå¯¹å…¶ä»–ä¼˜åŒ–ç®—æ³•çš„ä¸€ä¸ªè¡¥å……ã€‚

**å°ç»“**: æœ¬èŠ‚æå‡ºäº†é‡è¦çš„Proposition, é˜è¿°äº†ç®—æ³•2å’Œç®—æ³•1çš„ç­‰æ•ˆæ€§, å› æ­¤Pop-Artç®—æ³•å¯ä»¥ä½œä¸ºå¯¹å…¶ä»–ä¼˜åŒ–ç®—æ³•çš„ä¸€ä¸ªè¡¥å……ã€‚

### æ€»ç»“ä¸æ€è€ƒ

æœ¬æ–‡è€ƒè™‘å¼ºåŒ–å­¦ä¹ ä¸­targetè·¨åº¦å¤§è€Œå¯¼è‡´æ¨¡å‹å­¦ä¹ æ•ˆæœå·®çš„é—®é¢˜è¿›è¡Œäº†ç ”ç©¶, æå‡ºäº†Pop-Artç®—æ³•è§£å†³è¯¥é—®é¢˜ã€‚å…¶ç®—æ³•çš„æ ¸å¿ƒåŒ…æ‹¬ä¸¤ä¸ªéƒ¨åˆ†: 1) Pop (Presering outputs precisely): å³æ— è®ºæ ‡å‡†åŒ–å‚æ•°å¦‚ä½•å˜åŒ–, å·²æœ‰çš„è¾“å‡ºä¸ä¼šå˜åŒ–; 2) Art (Adaptive rescaling target): å³è‡ªé€‚åº”ç›®æ ‡å€¼æ”¾ç¼©, ä¿éšœæ–°çš„ç›®æ ‡å€¼èƒ½åˆç†çš„æ”¾ç¼©æ ‡å‡†åŒ–ã€‚æ­¤å¤–, æœ¬æ–‡è¿˜åˆ†æäº†targetè·¨åº¦å¤§å¯¼è‡´æ¨¡å‹å­¦ä¹ æ•ˆæœå·®çš„åŸå› : targetçš„è·¨åº¦å½±å“å°†"å¹³æ–¹å¼"åœ°å½±å“lower layerçš„å‚æ•°æ›´æ–°ã€‚åœ¨æ­¤åŸºç¡€ä¸Š, ä½œè€…æå‡ºäº†é’ˆå¯¹lower layeræ›´æ–°çš„ä¿®æ­£ç®—æ³•, å¹¶è¯æ˜äº†è¯¥ç®—æ³•ä¸å‰è¿°Pop-Artç®—æ³•çš„ç­‰æ•ˆæ€§ã€‚

**ç–‘é—®**: å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­å¤šä¸ºmini-batchçš„è®­ç»ƒæ¨¡å¼, é‚£ä¹ˆåœ¨éšæœºé€‰å–çš„batchä¸­å¯èƒ½å‡ºç°å¦‚ä¸‹çš„æƒ…å†µ: $[ a_1, \ldots, a_n, b_1, \ldots, b_m]$ã€‚å…¶ä¸­$a$, $b$åºåˆ—åˆ†åˆ«åœ¨ä¸åŒçš„èŒƒå›´, åŒç»„ä¸­å½¼æ­¤å·®å¼‚ä»…ä½“ç°åœ¨å°æ•°ç‚¹å3ä½, è€Œä¸¤ç»„é—´çš„å·®å¼‚ä¸ºä¸ªä½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹, æ ‡å‡†åŒ–å¤„ç†æ˜¯å¦èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†ç»„å†…çš„å·®å¼‚å‘¢ï¼Ÿæ­¤batchçš„å¤„ç†æ˜¯å¦åº”è¯¥å°†æ ·æœ¬é€ä¸€è¾“å…¥è¿›è¡Œå¤„ç†ï¼Ÿ

**æ­¤ç–‘é—®åœ¨æ–‡ç« çš„é™„åŠ éƒ¨åˆ†ä¸­æœ‰æ‰€æ¶‰åŠ, å³éœ€è¦æ›¿æ¢"Art"éƒ¨åˆ†ã€‚**