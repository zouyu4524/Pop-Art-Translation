## è®ºæ–‡ç¿»è¯‘: Learning values across many orders of magnitude

Hado van Hasselt, Arthur Guez, Matteo Hessel, Volodymyr Mnih, David Silver. "Learning values across many orders of magnitude." In *Advances in Neural Information Processing Systems*, pp. 4287-4295. 2016. [[pdf](https://arxiv.org/pdf/1602.07714)]

### ç¬¦å·ç³»ç»Ÿ

| ç¬¦å· 	| å«ä¹‰ | ç¬¦å· | å«ä¹‰ |
| :---: | :--- | :---: | :--- |
| <img alt="$Y_t$" src="svgs/39e731509d35a821a251fe62866ee4a4.svg" align="middle" width="14.509275000000004pt" height="22.46574pt"/> | ç›®æ ‡ (target) | <img alt="${\bf W}$" src="svgs/ce01fa56c40e0a3b42fb4a4c6c6d67fe.svg" align="middle" width="19.805940000000003pt" height="22.557149999999986pt"/>  | æ ‡å‡†åŒ–å±‚æƒé‡ | 
| <img alt="$\tilde{Y}_t$" src="svgs/6dd56c2803c2d23d14a979df7224a25d.svg" align="middle" width="14.509275000000004pt" height="30.267599999999987pt"/> | æ ‡å‡†åŒ–åçš„ç›®æ ‡ | <img alt="${\boldsymbol b}$" src="svgs/d86c2b3d62d848f120253b393984a5bb.svg" align="middle" width="8.561685000000002pt" height="22.831379999999992pt"/> | æ ‡å‡†åŒ–å±‚bias |
| <img alt="${\boldsymbol \Sigma}_t$" src="svgs/7f546d8bf5152d18c9ab0724628d1a5e.svg" align="middle" width="18.618765000000003pt" height="22.557149999999986pt"/>, <img alt="${\boldsymbol \mu}_t$" src="svgs/84d9637961b880dea8afc1447f1cc6ca.svg" align="middle" width="16.601970000000005pt" height="14.61206999999998pt"/> | å­¦ä¹ åˆ°çš„*scale*å’Œ*shift* | <img alt="$h_{\boldsymbol \theta}$" src="svgs/b021405126b65c0e04c15d944d1f134f.svg" align="middle" width="17.113965000000004pt" height="22.831379999999992pt"/> | æ¨¡å‹å‚æ•° | 
| <img alt="$g(\cdot)$" src="svgs/a0377869f11bf54f0a0e8ac338ecea79.svg" align="middle" width="25.782075000000003pt" height="24.65759999999998pt"/> | æ ‡å‡†åŒ–å‡½æ•° | <img alt="$f(\cdot)$" src="svgs/8824595f458085fe3bf467c4228300fc.svg" align="middle" width="27.169065pt" height="24.65759999999998pt"/> | åŸå‡½æ•°    |
 
### ç¬¬äºŒç«  Adaptive normalization with Pop-Art

> We propose to normalize the targets <img alt="$Y_t$" src="svgs/39e731509d35a821a251fe62866ee4a4.svg" align="middle" width="14.509275000000004pt" height="22.46574pt"/>, where the normalization is learned **separately** from the approximating function. We consider an affine transformation of the targets

<p align="center"><img alt="$$&#10;\tilde{Y}_ t = {\boldsymbol \Sigma}_ t^{-1} (Y_ t - {\boldsymbol \mu}_ t),&#10;$$" src="svgs/22fc937d416e5d5eba720ba16636ce1f.svg" align="middle" width="138.74784pt" height="19.24329pt"/></p>

æœ¬æ–‡çš„æ ¸å¿ƒæ€è·¯æ˜¯å°†å­¦ä¹ **æ ‡å‡†åŒ–**ä¸**å‡½æ•°æ‹Ÿåˆ**ä¸¤ä¸ªè¿‡ç¨‹åˆ†ç¦»å¼€, å…·ä½“åˆ†ç¦»çš„æ–¹å¼æ˜¯åœ¨åŸæœ‰çš„å‡½æ•°æ‹Ÿåˆç½‘ç»œä¸‹æ¥ä¸€å±‚**affine transformation**(çº¿æ€§å˜æ¢)ã€‚åŸæœ‰çš„å‡½æ•°è´Ÿè´£å­¦ä¹ å‡½æ•°æ‹Ÿåˆ, è€Œä¸‹æ¥çš„ä¸€å±‚çº¿æ€§å˜æ¢å±‚è´Ÿè´£"è·Ÿè¸ª"|å­¦ä¹ æ ‡å‡†åŒ–çš„å‚æ•°ã€‚

> We can then define a loss on a normalized function <img alt="$g(X_t)$" src="svgs/b76f93b526e06c1b9208513d17d87503.svg" align="middle" width="40.62234pt" height="24.65759999999998pt"/> and the normalized target <img alt="$\tilde{Y}_ T$" src="svgs/73ac43eb7799407056d01ec72c6c80e7.svg" align="middle" width="19.077135000000002pt" height="30.267599999999987pt"/>. THe unnormalized approximation for any input <img alt="$x$" src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg" align="middle" width="9.395100000000005pt" height="14.155350000000013pt"/> is then given by <img alt="$f(x) = {\boldsymbol \Sigma} g(x) + {\boldsymbol \mu}$" src="svgs/1d9424936f715c8f631977f52b3f57a2.svg" align="middle" width="129.906645pt" height="24.65759999999998pt"/>, where <img alt="$g$" src="svgs/3cf4fbd05970446973fc3d9fa3fe3c41.svg" align="middle" width="8.430510000000004pt" height="14.155350000000013pt"/> is the *normalized function* and <img alt="$f$" src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg" align="middle" width="9.817500000000004pt" height="22.831379999999992pt"/> is the *unnormalized function*.

> Thereby, we decompose the problem of learning an appropriate normalization from learning the specific shape of the function. The two properties that we want to simultaneosly achieve are

> (**ART**) to update scale <img alt="${\boldsymbol \Sigma}$" src="svgs/121d8877038c55b6166a6e22acc96149.svg" align="middle" width="13.652925000000005pt" height="22.557149999999986pt"/> and shift <img alt="${\boldsymbol \mu}$" src="svgs/4e52c06a410e541f12a7a783da1aa80d.svg" align="middle" width="11.636295000000004pt" height="14.61206999999998pt"/> such that <img alt="${\boldsymbol \Sigma}^{-1} (Y - {\boldsymbol \mu})$" src="svgs/ed3b22e3ba194b711ab8bab28a300df6.svg" align="middle" width="89.01057pt" height="29.260439999999992pt"/> is approproately normalized, and  
> (**POP**) to preserve the outputs of the unnormalized function when we change the scale and shift.

å…¶ä¸­**ART**è´Ÿè´£å­¦ä¹ æ ‡å‡†åŒ–å‚æ•°, è€Œ**POP**è´Ÿè´£ä¿éšœä»»ä½•çš„æ ‡å‡†åŒ–å‚æ•°ä¸‹æ€»èƒ½ä»æ ‡å‡†åŒ–åçš„ç»“æœå¤åŸåŸå§‹çš„è¾“å‡ºã€‚æœ‰äº†ä¸¤ç‚¹çš„ç»“åˆå°±èƒ½å®ç°é’ˆå¯¹ä»»æ„è¾“å‡ºæƒ…å†µä¸‹ç”¨åˆé€‚çš„æ ‡å‡†åŒ–å‚æ•°è¿›è¡Œnormalization, ä¸æ­¤åŒæ—¶ä¿éšœä¸ç ´åå·²ç»å­¦ä¹ åˆ°å‡½æ•°æ‹Ÿåˆç»“æœã€‚

#### 2.1 POP: Preserving outputs precisely

> Unless care is taken, repeated updates to the normalization might make learning harder rather than easier because the normalized targets become non-stationary. More importantly, whenever we adapt the normalization based on a certain target, this would simultaneously change the output of the unnormalized function of all inputs.

æœ¬æ®µè§£é‡Šäº†æå‡º**POP**çš„motivationã€‚å¯æƒ³è€ŒçŸ¥, å¦‚æœæ— æ³•ä¿éšœåŸå§‹ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰çš„targetä¸èƒ½è¢«å¤åŸçš„è¯, é‚£ä¹ˆä¸€æ—¦åœ¨å­¦ä¹ æ—¶é‡‡ç”¨äº†ä¸åŒçš„æ ‡å‡†åŒ–å‚æ•°, åŸæœ‰å­¦ä¹ åˆ°çš„æ¨¡å‹å°†å—åˆ°å½±å“ï¼ˆç ´åï¼‰ã€‚ä¸¾ä¾‹è€Œè¨€, å‡è®¾é¦–å…ˆå­¦ä¹ åˆ°çš„æ¨¡å‹æ˜¯é€šè¿‡åŸå§‹targetèŒƒå›´åœ¨<img alt="$[0, 1]$" src="svgs/e88c070a4a52572ef1d5792a341c0900.svg" align="middle" width="32.87674500000001pt" height="24.65759999999998pt"/>å†…, è€Œåå‡ºç°çš„æ ·æœ¬targetèŒƒå›´åœ¨<img alt="$[10, 100]$" src="svgs/a343cf36f4495c31070af031022eac92.svg" align="middle" width="57.534510000000004pt" height="24.65759999999998pt"/>, å°†å…¶æ ‡å‡†åŒ–è‡³<img alt="$[0, 1]$" src="svgs/e88c070a4a52572ef1d5792a341c0900.svg" align="middle" width="32.87674500000001pt" height="24.65759999999998pt"/>å†…åä½œä¸ºæ–°çš„æ ·æœ¬è®­ç»ƒå·²æœ‰æ¨¡å‹, é‚£ä¹ˆåŸæ¥æ¨¡å‹çš„æ‰€ç»™å‡ºçš„<img alt="$[0, 1]$" src="svgs/e88c070a4a52572ef1d5792a341c0900.svg" align="middle" width="32.87674500000001pt" height="24.65759999999998pt"/>èŒƒå›´ç»“æœå°†å—åˆ°"æ³¢åŠ"ã€‚

> The **only way** to avoid changing all outputs of the unnormalized function whenever we update the scale and shift is by changing the normalized function <img alt="$g$" src="svgs/3cf4fbd05970446973fc3d9fa3fe3c41.svg" align="middle" width="8.430510000000004pt" height="14.155350000000013pt"/> itself simultaneously. The goal is the preserve the outputs from before the change of normalization, for all inputs. This prevents the normalization from affecting the approximation, which is appropriate because its objective is solely to make learning easier, and to leave solving the approximation itself to the optimization algorithm.

è§£å†³æ­¤é—®é¢˜çš„å”¯ä¸€åŠæ³•å°±æ˜¯åœ¨æ›´æ–°æ ‡å‡†åŒ–å‚æ•°çš„åŒæ—¶æ›´æ–°åŸæœ‰æ ‡å‡†åŒ–å‰çš„è¾“å‡ºã€‚æ ‡å‡†åŒ–çš„ç›®çš„åœ¨äºä½¿å¾—è®­ç»ƒé›†åœ¨å·®ä¸å¤šçš„è§„æ¨¡å†…ä»è€Œä½¿æ¨¡å‹è®­ç»ƒæ›´ä¸ºå®¹æ˜“, éœ€è¦é˜²æ­¢æ ‡å‡†åŒ–å½±å“æ¨¡å‹è®­ç»ƒæœ¬èº«ã€‚

> Without loss of generality the unnormalized function can be written as  
<p align="center"><img alt="$$&#10;f_ { {\boldsymbol \theta, \Sigma}, {\bf W}, {\boldsymbol b} } (x) \equiv {\boldsymbol \Sigma} g_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (x) + {\boldsymbol \mu} \equiv {\boldsymbol \Sigma} ( {\bf W} h_ { {\boldsymbol \theta} } (x) + {\boldsymbol b}) + {\boldsymbol \mu},&#10;$$" src="svgs/4c683ec967181adeab480d0cb8f72107.svg" align="middle" width="390.9444pt" height="17.031959999999998pt"/></p> 

å…¶ä¸­<img alt="$g_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (x) = {\bf W} h_ { {\boldsymbol \theta} } (x) + {\boldsymbol b}$" src="svgs/951da717c5fac63397630ae2cb8d54f3.svg" align="middle" width="178.956855pt" height="24.65759999999998pt"/> ä¸ºæ ‡å‡†åŒ–å‡½æ•°, è€Œ<img alt="$h_ { {\boldsymbol \theta} } (x)$" src="svgs/6a1250dbd9366616ae954c3472309f2f.svg" align="middle" width="40.11612pt" height="24.65759999999998pt"/> æ˜¯å‡½æ•°æ‹Ÿåˆçš„ç½‘ç»œ(non-linear)ã€‚

> It is not uncommon for deep neural networks to end in a linear layer, and the <img alt="$h_ { {\boldsymbol \theta} }$" src="svgs/0c212e3dd00bcc0be7e55ad560026ac6.svg" align="middle" width="17.113965000000004pt" height="22.831379999999992pt"/> can be the output of the last (hidden) layer of non-linearities. Alternatively, we can always add a square linear layer to any non-linear function <img alt="$h_ { {\boldsymbol \theta} }$" src="svgs/0c212e3dd00bcc0be7e55ad560026ac6.svg" align="middle" width="17.113965000000004pt" height="22.831379999999992pt"/> to ensure this constraint, for instance initialized as <img alt="${\bf W}_ 0 = {\bf I}$" src="svgs/01268c569b6146a9845b6c3bef8f0841.svg" align="middle" width="56.266980000000004pt" height="22.557149999999986pt"/> and <img alt="${\boldsymbol b}_ 0 = {\boldsymbol 0}$" src="svgs/990ea5833bad1ce4e52b1647978ce072.svg" align="middle" width="47.305665000000005pt" height="22.831379999999992pt"/>.

æœ¬æ®µè§£é‡Šäº†ç½‘ç»œæ¶æ„, ä¸€èˆ¬è€Œè¨€DNNçš„æœ€åä¸€å±‚ï¼ˆè¾“å‡ºå±‚ï¼‰å‡å…·å¤‡çº¿æ€§æ¿€æ´», å³æ»¡è¶³ä»¥ä¸Šçš„æ¡ä»¶; è‹¥ä¸ç„¶, åˆ™å¯ä»¥åœ¨åŸæœ‰ç½‘ç»œçš„åŸºç¡€ä¸ŠåŠ ä¸Šä¸€ä¸ªçº¿æ€§å±‚(æœ¬å±‚å‚æ•°è§„æ¨¡ä¸º<img alt="$k\times k | k$" src="svgs/69b48152de429efd0c9094b4cab7b11d.svg" align="middle" width="51.88359pt" height="24.65759999999998pt"/>, <img alt="$k$" src="svgs/63bb9849783d01d91403bc9a5fea12a2.svg" align="middle" width="9.075495000000004pt" height="22.831379999999992pt"/>ä¸ºè¾“å‡ºçš„å±‚çš„ç¥ç»å…ƒæ•°é‡), ä¸æ”¹å˜ç½‘ç»œæ¶æ„çš„åŒæ—¶æ»¡è¶³äº†ä»¥ä¸Šçš„æ¡ä»¶ã€‚

> **Proposition 1.** *Consider a function <img alt="$f: \mathcal{R}^n \rightarrow \mathcal{R}^k$" src="svgs/d2662946a3ae77ef0029b52c0aa631a5.svg" align="middle" width="93.16362pt" height="27.91271999999999pt"/> as*
<p align="center"><img alt="$$&#10;f_ { {\boldsymbol \theta, \Sigma}, {\bf W}, {\boldsymbol b} } (x) \equiv {\boldsymbol \Sigma} ( {\bf W} h_ { {\boldsymbol \theta} } (x) + {\boldsymbol b}) + {\boldsymbol \mu},&#10;$$" src="svgs/a662b947928b8d220f6909ca7e151d3a.svg" align="middle" width="255.18405pt" height="17.031959999999998pt"/></p>  

> *where <img alt="$h_ {\boldsymbol \theta}: \mathcal{R}^n \rightarrow \mathcal{R}^m$" src="svgs/0f290327f70b5c997fc4b2496a20e507.svg" align="middle" width="105.680685pt" height="22.831379999999992pt"/> is any non-linear function of <img alt="$x\in \mathcal{R}^n$" src="svgs/004ab5a9f19ea49b06deb310e0407c1e.svg" align="middle" width="51.543855pt" height="22.46574pt"/>, <img alt="${\boldsymbol \Sigma}$" src="svgs/121d8877038c55b6166a6e22acc96149.svg" align="middle" width="13.652925000000005pt" height="22.557149999999986pt"/> is a <img alt="$k\times k$" src="svgs/649b7f492ffd7b6c6f42429f3fe29451.svg" align="middle" width="38.242050000000006pt" height="22.831379999999992pt"/> matrix, <img alt="${\boldsymbol \mu}$" src="svgs/4e52c06a410e541f12a7a783da1aa80d.svg" align="middle" width="11.636295000000004pt" height="14.61206999999998pt"/> and <img alt="${\boldsymbol b}$" src="svgs/d86c2b3d62d848f120253b393984a5bb.svg" align="middle" width="8.561685000000002pt" height="22.831379999999992pt"/> are <img alt="$k$" src="svgs/63bb9849783d01d91403bc9a5fea12a2.svg" align="middle" width="9.075495000000004pt" height="22.831379999999992pt"/>-element vectors, and <img alt="${\bf W}$" src="svgs/ce01fa56c40e0a3b42fb4a4c6c6d67fe.svg" align="middle" width="19.805940000000003pt" height="22.557149999999986pt"/> is a <img alt="$k\times m$" src="svgs/ef924289610252b6b10f485f9cc2a2d3.svg" align="middle" width="43.599765000000005pt" height="22.831379999999992pt"/> matrix. Consider any change of the scale and shift parameters from <img alt="${\boldsymbol \Sigma}$" src="svgs/121d8877038c55b6166a6e22acc96149.svg" align="middle" width="13.652925000000005pt" height="22.557149999999986pt"/> to <img alt="${\boldsymbol \Sigma}_ \text{new}$" src="svgs/626f14c3ae1babc3216165ecbfab765d.svg" align="middle" width="36.318645000000004pt" height="22.557149999999986pt"/> and from <img alt="${\boldsymbol \mu}$" src="svgs/4e52c06a410e541f12a7a783da1aa80d.svg" align="middle" width="11.636295000000004pt" height="14.61206999999998pt"/> to <img alt="${\boldsymbol \mu}_ \text{new}$" src="svgs/759b0ba96f2cc8322bbbf9dde93e366f.svg" align="middle" width="34.30185pt" height="14.61206999999998pt"/>, where <img alt="${\boldsymbol \Sigma}_ \text{new}$" src="svgs/626f14c3ae1babc3216165ecbfab765d.svg" align="middle" width="36.318645000000004pt" height="22.557149999999986pt"/> is non-singular. If we then additionally change the parameters <img alt="${\bf W}$" src="svgs/ce01fa56c40e0a3b42fb4a4c6c6d67fe.svg" align="middle" width="19.805940000000003pt" height="22.557149999999986pt"/> and <img alt="${\boldsymbol b}$" src="svgs/d86c2b3d62d848f120253b393984a5bb.svg" align="middle" width="8.561685000000002pt" height="22.831379999999992pt"/> to <img alt="${\bf W}_ \text{new}$" src="svgs/9f1325fa9976a0e80cb78e9afb4005c1.svg" align="middle" width="42.471495pt" height="22.557149999999986pt"/> and <img alt="${\boldsymbol b}_ \text{new}$" src="svgs/97dba530b5af389ba37d72b44c5b15be.svg" align="middle" width="31.227240000000002pt" height="22.831379999999992pt"/>, defined by*  

<p align="center"><img alt="$$ &#10;{\bf W}_ \text{new} = {\boldsymbol \Sigma}_ \text{new}^{-1} {\boldsymbol \Sigma} {\bf W} \quad \text{and} \quad {\boldsymbol b}_ \text{new} = {\boldsymbol \Sigma}_ \text{new}^{-1} \left( {\boldsymbol \Sigma b + \mu - \mu}_ \text{new} \right)&#10;$$" src="svgs/95f9bf2f5e8978a721f10fa8b2b81046.svg" align="middle" width="405.10634999999996pt" height="18.73971pt"/></p>  

> *then the outputs of the unnormalized function <img alt="$f$" src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg" align="middle" width="9.817500000000004pt" height="22.831379999999992pt"/> are preserved precisely in the sense that*  

<p align="center"><img alt="$$&#10;f_ { {\boldsymbol \theta, \Sigma}, {\bf W}, {\boldsymbol b} } (x) = f_ { {\boldsymbol \theta, \Sigma}_ \text{new}, {\bf W}_ \text{new}, {\boldsymbol b}_ \text{new} } (x), \quad \forall x.&#10;$$" src="svgs/5a763cc670ed583d67ce69be8bc88e51.svg" align="middle" width="292.12755pt" height="17.853824999999997pt"/></p>

ä»¥ä¸Šçš„Propositionè¡¨æ˜åªè¦é‡‡ç”¨**é€‚å½“çš„**æ ‡å‡†åŒ–å‚æ•°å˜æ¢æ–¹æ¡ˆæ€»èƒ½ä¿è¯åŸå‡½æ•°æ‹Ÿåˆçš„ç»“æœä¿æŒä¸å˜ã€‚å…¶ä¸­é€‚å½“çš„å˜åŒ–æ–¹æ¡ˆå³ä»¥ä¸Šç»™å‡ºçš„<img alt="${\bf W}_ \text{new}$" src="svgs/9f1325fa9976a0e80cb78e9afb4005c1.svg" align="middle" width="42.471495pt" height="22.557149999999986pt"/>ä¸<img alt="${\boldsymbol b}_ \text{new}$" src="svgs/97dba530b5af389ba37d72b44c5b15be.svg" align="middle" width="31.227240000000002pt" height="22.831379999999992pt"/>çš„æ›´æ–°æ–¹æ¡ˆã€‚

> For the special case of scalar scale and shift, with <img alt="${\boldsymbol \Sigma} \equiv \sigma {\bf I}$" src="svgs/f3a8224dc5329fe285b71e5b41d4769b.svg" align="middle" width="52.72245pt" height="22.557149999999986pt"/> and <img alt="${\boldsymbol \mu} \equiv \mu {\boldsymbol 1}$" src="svgs/19071f7482fd4538727794dc3e7c0a19.svg" align="middle" width="52.91088pt" height="21.18732pt"/>, the updates to <img alt="${\bf W}$" src="svgs/ce01fa56c40e0a3b42fb4a4c6c6d67fe.svg" align="middle" width="19.805940000000003pt" height="22.557149999999986pt"/> and <img alt="${\boldsymbol b}$" src="svgs/d86c2b3d62d848f120253b393984a5bb.svg" align="middle" width="8.561685000000002pt" height="22.831379999999992pt"/> become <img alt="${\bf W}_ \text{new} = (\sigma/\sigma_\text{new}) {\bf W}$" src="svgs/ee1ce1a404b265b7ffdf0c587e2356ea.svg" align="middle" width="148.531185pt" height="24.65759999999998pt"/> and <img alt="${\boldsymbol b}_ \text{new} = (\sigma {\boldsymbol b} + {\boldsymbol \mu} - {\boldsymbol \mu}_ \text{new})/\sigma_ \text{new}$" src="svgs/194029dc8db2fb66141948b2e96974c2.svg" align="middle" width="212.16310499999997pt" height="24.65759999999998pt"/>. After updating the scale and shift we can update the output of the normalized function <img alt="$g_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (X_ t)$" src="svgs/d5d75d4c4c6d5d6e8a50089ee85f3f6b.svg" align="middle" width="78.475485pt" height="24.65759999999998pt"/> toward the normalized <img alt="$\tilde{Y}_ t$" src="svgs/a991a484bfbd0041e2b3cb9928640a20.svg" align="middle" width="14.509275000000004pt" height="30.267599999999987pt"/>, using any learning algorithm.

ä»¥ä¸Šç»™å‡ºäº†ç‰¹ä¾‹, å³å½“scaleå’Œshiftå‡ä¸ºæ ‡é‡ã€‚æŒ‰ä»¥ä¸Šè§„åˆ™æ›´æ–°äº†æ ‡å‡†åŒ–å‚æ•°å, æ ¹æ®**Proposition 1**å¯çŸ¥åŸæ¥çš„è¾“å‡ºåœ¨æ–°çš„æ ‡å‡†åŒ–å‚æ•°ä¸‹å¹¶ä¸ä¼šæ”¹å˜, è€Œæ–°çš„è®­ç»ƒæ•°æ®é€šè¿‡æ–°çš„æ ‡å‡†åŒ–å‚æ•°å¤„ç†åå³å¯ç”¨äºå¯¹å‡½æ•°æ‹Ÿåˆæ¨¡å‹(å³<img alt="$h_ { {\boldsymbol \theta} }$" src="svgs/0c212e3dd00bcc0be7e55ad560026ac6.svg" align="middle" width="17.113965000000004pt" height="22.831379999999992pt"/>)çš„è®­ç»ƒã€‚

<p align="center">
<img src="https://user-images.githubusercontent.com/16682999/64672136-33788080-d49d-11e9-9771-bc07a48e99b6.png" alt="algorithm 1" width="800">
</p>

> Algorithm 1 is an example implementation of SGD with Pop-Art for a squared loss. It can be generalized easily to any other loss by changin the definition of <img alt="${\boldsymbol \delta}$" src="svgs/ef2db80fd6c60a9bf24092a1f40a1215.svg" align="middle" width="9.212280000000005pt" height="22.831379999999992pt"/>. Notice that <img alt="${\bf W}$" src="svgs/ce01fa56c40e0a3b42fb4a4c6c6d67fe.svg" align="middle" width="19.805940000000003pt" height="22.557149999999986pt"/> and <img alt="${\boldsymbol b}$" src="svgs/d86c2b3d62d848f120253b393984a5bb.svg" align="middle" width="8.561685000000002pt" height="22.831379999999992pt"/> are updated twice: first to adapt to the new scale and shift to preserve the outputs of the function, and then by SGD. The order of these updates is important because it allows us to use the new normalization immediately in the subsequent SGD update.

åœ¨æ­¤åŸºç¡€ä¸Š, ç®—æ³•1ä»¥MSEä¸ºlosså‡½æ•°, SGDä¸ºoptimizerä¸ºä¾‹é˜è¿°äº†å¦‚ä½•å®ç°Pop-Artç®—æ³•ã€‚å…¶ä¸­ä¸»è¦åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µ:  

- æ›´æ–°æ ‡å‡†åŒ–å‚æ•°(Pop)  
- æ›´æ–°å‡½æ•°æ‹Ÿåˆæ¨¡å‹å†…å±‚(<img alt="$h_ { {\boldsymbol \theta} }$" src="svgs/0c212e3dd00bcc0be7e55ad560026ac6.svg" align="middle" width="17.113965000000004pt" height="22.831379999999992pt"/>), (ç®—æ³•ä¸­çº¢æ¡†æ ‡æ³¨éƒ¨åˆ†)  
- ç”±SDGæ›´æ–°æ ‡å‡†åŒ–å±‚å‚æ•°

**å°ç»“**: æœ¬èŠ‚ä¸»è¦é˜è¿°äº†Pop-Artç®—æ³•ä¸­çš„Popéƒ¨åˆ†, å³å¦‚ä½•æ›´æ–°æ ‡å‡†åŒ–å‚æ•°ä»¥ä¿è¯åç»­çš„è®­ç»ƒå¯ä»¥ä¿éšœä¸å½±å“å·²æœ‰è¾“å‡ºçš„ç»“æœã€‚å¦ä¸€æ–¹é¢, é€šè¿‡ç®—æ³•1ç»™å‡ºäº†ç»“åˆPop-Artçš„ç½‘ç»œæ›´æ–°æµç¨‹, å…¶ä¸­æ ‡å‡†åŒ–å±‚çš„å‚æ•°å°†è¢«æ›´æ–°ä¸¤æ¬¡, ç¬¬ä¸€æ¬¡ä¸ºä¿éšœPop, ç¬¬äºŒæ¬¡ä¸ºä¼˜åŒ–ç®—æ³•ä¸‹çš„å‚æ•°æ›´æ–°ã€‚

#### 2.2 ART: Adaptively rescaling targets

å‰ä¸€èŠ‚ä¸­ä»‹ç»äº†æ ‡å‡†åŒ–å‚æ•°æ›´æ–°è¿‡ç¨‹ä¸­ä¿éšœå·²æœ‰è¾“å‡ºä¸å˜çš„åŸºæœ¬æ€è·¯, è€Œæœ¬èŠ‚å°†å…·ä½“ç»™å‡ºå¦‚ä½•è¿›è¡Œæ ‡å‡†åŒ–, å³Art: Adaptively rescaling targetsçš„å†…æ¶µã€‚

> A natual choice is to normalize the targets to approximately have zero mean and unit variance. For clarity and conciseness, we consider scalar normalizations. If we have data <img alt="$\{ ( X_ i, Y_ i ) \}_ {i=1}^t$" src="svgs/42130d11734a175740b13e59c4e1f5bd.svg" align="middle" width="91.93239pt" height="26.086169999999992pt"/> up to some time <img alt="$t$" src="svgs/4f4f4e395762a3af4575de74c019ebb5.svg" align="middle" width="5.936155500000004pt" height="20.222069999999988pt"/>, we then may desire

<p align="center"><img alt="$$&#10;\begin{aligned}&#10;&amp; \sum_ {i=1}^t (Y_ i - \mu_ t) / \sigma_ t = 0 \quad &amp; \text{and} \quad &amp; \frac{1}{t} \sum_ {i=1}^t ( Y_ i - \mu_ t )^2 / \sigma_t ^2 = 1,\\&#10;\text{such that} \quad \mu_t &amp;=\frac{1}{t}\sum_ {i=1}^t Y_i \quad &amp; \text{and} \quad \sigma_t^2 &amp;=\frac{1}{t} \sum_ {i=1}^t Y_i^2 - \mu_t^2.&#10;\end{aligned}&#10;$$" src="svgs/6cdad7742bddda1b2cd56580ce6ca043.svg" align="middle" width="505.99559999999997pt" height="103.90446pt"/></p>

> This can be generalized to incremental updates

<p align="center"><img alt="$$&#10;\mu_t = (1-\beta_t) \mu_{t-1} +\beta_t Y_t~\text{and}~\sigma_t^2 = \nu_t - \mu_t^2, \text{where}~ \nu_t = (1 - \beta_t) \nu_{t-1} + \beta_t Y_t^2.&#10;$$" src="svgs/11923b8eedbbab19d238723ed28a64b9.svg" align="middle" width="542.7113999999999pt" height="18.312359999999998pt"/></p>

> Here <img alt="$\nu_t$" src="svgs/b96ce619e9618788ad604f351c957414.svg" align="middle" width="13.086150000000004pt" height="14.155350000000013pt"/> estimates the second moment of the targets and <img alt="$\beta_t\in [0, 1]$" src="svgs/0a550c41abe1f9062c064428cfaf16a3.svg" align="middle" width="68.05359pt" height="24.65759999999998pt"/> is a step size. If <img alt="$\nu_t -\mu_t^2$" src="svgs/6d5125c6f26b74912240f25615270ae8.svg" align="middle" width="50.45667000000001pt" height="26.76201000000001pt"/> is positive initially then it will always remain so, although to avoid issues with numerical precision it can be useful to enforce a lower bound explicitly by requiring <img alt="$\nu_t -\mu_t^2 \geq \epsilon$" src="svgs/d86aecf0c5cf788e0d6a62ad52f807e7.svg" align="middle" width="79.86858pt" height="26.76201000000001pt"/> with <img alt="$\epsilon &gt;0$" src="svgs/b7e0fd4eae1532edec52a4554babe404.svg" align="middle" width="36.809355000000004pt" height="21.18732pt"/>. For full equivalence to above one we can use <img alt="$\beta_t = 1/t$" src="svgs/74f162db9f6cabad0259229548d5bfa1.svg" align="middle" width="59.37789pt" height="24.65759999999998pt"/>. If <img alt="$\beta_t = \beta$" src="svgs/722fa5db799a3e8c44a77b2bb5d5f958.svg" align="middle" width="47.16888pt" height="22.831379999999992pt"/> is constant we get exponential moving averages, placing more weight on recent data points which is appropriate in non-stationary settings.

è‡ªç„¶åœ°, æ ‡å‡†åŒ–çš„æ€è·¯æ˜¯å°†ç°æœ‰çš„æ•°æ®ç»Ÿä¸€åˆ°å‡å€¼ä¸º0, æ–¹å·®ä¸º1çš„è§„æ¨¡, è€ƒè™‘åˆ°æ•°æ®æœ¬èº«æ˜¯æºæºä¸æ–­å‡ºç°çš„, æ›´ä¸€èˆ¬åœ°å¯ä»¥ç”¨ä»¥ä¸Šçš„å¢é‡å¼æ›´æ–°æ–¹å¼updateå‡å€¼(<img alt="$\mu_t$" src="svgs/87eefe082e181864d1321025c2705ecd.svg" align="middle" width="14.870790000000003pt" height="14.155350000000013pt"/>)å’Œæ ‡å‡†å·®(<img alt="$\sigma_t$" src="svgs/5ba3f1b75931f41283dac26b10c8c182.svg" align="middle" width="14.358960000000003pt" height="14.155350000000013pt"/>)ã€‚ä¸¤ç§ç‰¹æ®Šæƒ…å†µ: 1) <img alt="$\beta_t = 1/t$" src="svgs/74f162db9f6cabad0259229548d5bfa1.svg" align="middle" width="59.37789pt" height="24.65759999999998pt"/>, åˆ™æ¯ä¸ªæ ·æœ¬çš„æƒé‡ä¸€è‡´; 2) <img alt="$\beta_t=\beta$" src="svgs/867dbaac43e71a1520ff55b9f46957d0.svg" align="middle" width="47.16888pt" height="22.831379999999992pt"/>ä¸ºå®šå€¼, åˆ™æ„å‘³ç€æŒ‡æ•°å¼çš„æ»‘åŠ¨å¹³å‡, æœ€è¿‘çš„æ ·æœ¬å…·æœ‰æ›´é«˜çš„æƒé‡, é€‚ç”¨äºnon-stationaryçš„æƒ…å†µã€‚å¦ä¸€æ–¹é¢, ä»æ•°å€¼ç²¾åº¦è€ƒè™‘, æœ‰å¿…è¦ä¸ºæ›´æ–°çš„<img alt="$\sigma_t^2$" src="svgs/a7fb002779eacea5794d0e6ec7ae0753.svg" align="middle" width="16.535475000000005pt" height="26.76201000000001pt"/>(å³<img alt="$\nu_t - \mu_t^2$" src="svgs/4a2439caf4a3e9cf078b45183f574586.svg" align="middle" width="50.45667000000001pt" height="26.76201000000001pt"/>)è®¾ç½®ä¸€ä¸ªä¸‹é™, <img alt="$\epsilon$" src="svgs/7ccca27b5ccc533a2dd72dc6fa28ed84.svg" align="middle" width="6.672451500000003pt" height="14.155350000000013pt"/>, ä»¥é˜²å‡ºç°é™¤0çš„æƒ…å†µã€‚

> A constant <img alt="$\beta$" src="svgs/8217ed3c32a785f0b5aad4055f432ad8.svg" align="middle" width="10.165650000000005pt" height="22.831379999999992pt"/> has the additional benefit of never becoming negligibly small. Consider the first time a target is observed that is much larger than all previously observed targets. If <img alt="$\beta_t$" src="svgs/b2891c2d243ec9bb147096d2353d6bb4.svg" align="middle" width="14.263755000000003pt" height="22.831379999999992pt"/> is small, our statistics would adapt only slightly, and the resulting update may be large enough to harm the learning. If <img alt="$\beta_t$" src="svgs/b2891c2d243ec9bb147096d2353d6bb4.svg" align="middle" width="14.263755000000003pt" height="22.831379999999992pt"/> is not too small, the normalization can adapt to the large target before updating, potentially making learning more robust.

æœ¬æ®µè¿›ä¸€æ­¥è§£é‡Šäº†<img alt="$\beta$" src="svgs/8217ed3c32a785f0b5aad4055f432ad8.svg" align="middle" width="10.165650000000005pt" height="22.831379999999992pt"/>å–å®šå€¼çš„ä¸€ä¸ªä¼˜ç‚¹ã€‚å‡è®¾<img alt="$\beta$" src="svgs/8217ed3c32a785f0b5aad4055f432ad8.svg" align="middle" width="10.165650000000005pt" height="22.831379999999992pt"/>é€æ­¥é€’å‡è‡³è¾ƒå°å€¼æ—¶, è‹¥æ­¤æ—¶é¦–æ¬¡å‡ºç°ä¸€ä¸ªç›¸å¯¹å¤§çš„ç›®æ ‡å€¼, ç”±äº<img alt="$\beta$" src="svgs/8217ed3c32a785f0b5aad4055f432ad8.svg" align="middle" width="10.165650000000005pt" height="22.831379999999992pt"/>å¤ªå°, å¯¼è‡´æ–°çº³å…¥çš„æ ·æœ¬å¯¹å·²æœ‰çš„ç»Ÿè®¡é‡(<img alt="$\mu$" src="svgs/07617f9d8fe48b4a7b3f523d6730eef0.svg" align="middle" width="9.904950000000003pt" height="14.155350000000013pt"/>å’Œ<img alt="$\sigma$" src="svgs/8cda31ed38c6d59d14ebefa440099572.svg" align="middle" width="9.982995000000003pt" height="14.155350000000013pt"/>)çš„å½±å“å¾ˆå¾®å°, å¯ä»¥è¿‘ä¼¼è®¤ä¸ºç»Ÿè®¡é‡ä¸å˜, é‚£ä¹ˆæ ¹æ®ç®—æ³•1, å…¶ä¸­å…³é”®æ­¥éª¤1å¯¹ç»Ÿè®¡é‡çš„æ›´æ–°å°±å¯ä»¥å¿½ç•¥, æ­¤æ—¶ç¬¬äºŒæ­¥ä¸­ç”±äºæ–°çš„æ ·æœ¬targetå€¼è¿‡å¤§å¯¼è‡´å¯¹æ¨¡å‹çš„è®­ç»ƒäº§ç”Ÿå¤§çš„å½±å“ã€‚ç›¸å, å¦‚æœ<img alt="$\beta$" src="svgs/8217ed3c32a785f0b5aad4055f432ad8.svg" align="middle" width="10.165650000000005pt" height="22.831379999999992pt"/>ä¸å¤ªå°, è¿™æ ·çš„æƒ…å†µä¸‹, åœ¨å‡½æ•°æ‹Ÿåˆç½‘ç»œ(<img alt="$h_ { {\boldsymbol \theta} }$" src="svgs/0c212e3dd00bcc0be7e55ad560026ac6.svg" align="middle" width="17.113965000000004pt" height="22.831379999999992pt"/>)ä¹‹å‰, è¾ƒå¤§çš„targetå€¼å°†ç”±ç»Ÿè®¡é‡çš„æ›´æ–°è€Œå‰Šå¼±è¿›è€Œå¢å¼ºæ¨¡å‹æ•´ä½“çš„é²æ£’æ€§ã€‚

> **Proposition 2.** *When using updates above to adapt the normalization parameters <img alt="$\sigma$" src="svgs/8cda31ed38c6d59d14ebefa440099572.svg" align="middle" width="9.982995000000003pt" height="14.155350000000013pt"/> and <img alt="$\mu$" src="svgs/07617f9d8fe48b4a7b3f523d6730eef0.svg" align="middle" width="9.904950000000003pt" height="14.155350000000013pt"/>, the normalized targets are bounded for all <img alt="$t$" src="svgs/4f4f4e395762a3af4575de74c019ebb5.svg" align="middle" width="5.936155500000004pt" height="20.222069999999988pt"/> by*
<p align="center"><img alt="$$&#10;-\sqrt{(1 - \beta_t) / \beta_t} \leq (Y_t - \mu_t) / \sigma_t \leq \sqrt{(1 - \beta_t) / \beta_t}.&#10;$$" src="svgs/f97207fa1ebd5278b1ee1e0b387d9d5f.svg" align="middle" width="340.33725pt" height="19.726245pt"/></p>

> For instance, if <img alt="$\beta_t = \beta = 10^{-4}$" src="svgs/d49e092afac3c6a5cdaafb490c6b184f.svg" align="middle" width="102.35148000000001pt" height="26.76201000000001pt"/> for all <img alt="$t$" src="svgs/4f4f4e395762a3af4575de74c019ebb5.svg" align="middle" width="5.936155500000004pt" height="20.222069999999988pt"/>, then the normalized target is guaranteed to be in <img alt="$(-100, 100)$" src="svgs/6044cdaa4da611a15e9e83bb3c68b225.svg" align="middle" width="82.19211pt" height="24.65759999999998pt"/>. 

ä»¥ä¸Šçš„Propositionè¡¨æ˜äº†ä¸Šè¿°å¢é‡å¼ç»Ÿè®¡å‚æ•°æ›´æ–°ä¸‹è·å¾—çš„targetèŒƒå›´ä¸<img alt="$\beta_t$" src="svgs/b2891c2d243ec9bb147096d2353d6bb4.svg" align="middle" width="14.263755000000003pt" height="22.831379999999992pt"/>çš„å…³ç³», å¯¹äºå‚æ•°é€‰æ‹©å…·æœ‰ä¸€å®šæŒ‡å¯¼æ•ˆæœã€‚

> It is an open question whether it is uniformly best to normalize by mean and variance. In the appendix we discuss other normalization updates, based on percentiles and mini-batches, and derive correspondences between all of these.

ä»¥ä¸Šç»™å‡ºçš„æ˜¯å‡å€¼|æ ‡å‡†å·®çš„æ ‡å‡†åŒ–æ–¹å¼, è¯¥ç§æ–¹å¼æ˜¯å¦å…·æœ‰æ™®é€‚æ€§ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾å¼çš„é—®é¢˜ã€‚ä½œè€…åœ¨é™„å½•ä¸­è®¨è®ºäº†å…¶ä»–çš„æ ‡å‡†åŒ–æ–¹å¼, å°¤å…¶æ˜¯**mini-batches**å€¼å¾—å…³æ³¨ã€‚

**å°ç»“**: æœ¬èŠ‚ä»‹ç»äº†Pop-Artä¸­çš„"Art"éƒ¨åˆ†, å³è‡ªé€‚åº”targetç¼©æ”¾ã€‚æå‡ºäº†ä¸€ä¸ªå¢é‡å¼çš„æ ‡å‡†åŒ–æ–¹å¼, å…¶ä¸­é€šè¿‡å‚æ•°<img alt="$\beta_t$" src="svgs/b2891c2d243ec9bb147096d2353d6bb4.svg" align="middle" width="14.263755000000003pt" height="22.831379999999992pt"/>çš„ä¸åŒå–å€¼æ–¹å¼å¯ä»¥åº”å¯¹ä¸åŒçš„åœºæ™¯ã€‚é’ˆå¯¹target non-stationaryçš„æƒ…å½¢, å»ºè®®ä½¿ç”¨<img alt="$\beta_t$" src="svgs/b2891c2d243ec9bb147096d2353d6bb4.svg" align="middle" width="14.263755000000003pt" height="22.831379999999992pt"/>ä¸ºå®šå€¼, ä»¥å¢å¼ºæ¨¡å‹é²æ£’æ€§, ä¾¿äºåº”å¯¹targetå‡ºç°çªç„¶çš„å˜åŒ–ã€‚ï¼ˆå¦åˆ™, è‹¥<img alt="$\beta_t$" src="svgs/b2891c2d243ec9bb147096d2353d6bb4.svg" align="middle" width="14.263755000000003pt" height="22.831379999999992pt"/>éš<img alt="$t$" src="svgs/4f4f4e395762a3af4575de74c019ebb5.svg" align="middle" width="5.936155500000004pt" height="20.222069999999988pt"/>å‡å°, "å¼‚å¸¸"çš„targetå¯¹ç»Ÿè®¡å‚æ•°çš„å½±å“å°†å¾ˆå°, ä½†å¯¹æ¨¡å‹æ›´æ–°çš„"ä¼¤å®³"å´å¾ˆå¤§ã€‚ï¼‰

#### 2.3 An equivalence for stochastic gradient descent

> We now step back and analyze the effect of the magnitude of the errors on the gradients when using regular SDG. This analysis suggests a different normalization algorithm, which has an interesting correspondence to Pop-Art SGD.

åœ¨æœ¬èŠ‚ä¸­, ä½œè€…å°†è®¨è®ºè·¨åº¦å¹¿æ³›çš„targetå¯¹SGDç®—æ³•çš„å½±å“, å¹¶æå‡ºç›¸åº”çš„æ ‡å‡†åŒ–ç®—æ³•ã€‚è¯¥ç®—æ³•ä¸å‰è¿°çš„Pop-Art SGD (ç®—æ³•1)æœ‰ç€æœ‰è¶£çš„å…³è”ã€‚

> We consider SGD updates for an unnormalized multi-layer function of form <img alt="$f_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (X) = {\bf W} h_ { \boldsymbol \theta}(X) + {\boldsymbol b}$" src="svgs/b89ea407d35f61bf1f59e1d031b7c5a7.svg" align="middle" width="190.19170499999998pt" height="24.65759999999998pt"/>. The update for the weight matrix <img alt="${\bf W}$" src="svgs/ce01fa56c40e0a3b42fb4a4c6c6d67fe.svg" align="middle" width="19.805940000000003pt" height="22.557149999999986pt"/> is 

<p align="center"><img alt="$$&#10;{\bf W}_ t = {\bf W}_ {t-1} + \alpha_t {\boldsymbol \delta}_ t h_ { { \boldsymbol \theta}_ t} (X_t)^\intercal,&#10;$$" src="svgs/2a826400e4c702cadaec453e0adbcf57.svg" align="middle" width="209.47574999999998pt" height="16.438356pt"/></p>

> where <img alt="${\boldsymbol \sigma}_ t = f_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (X) - Y_t$" src="svgs/75dff7ee493e1457083b4efb36098b78.svg" align="middle" width="148.37427pt" height="24.65759999999998pt"/> is gradient of the squared loss, which we here call **unnormalized error**. The magnitude of this update depends linearly on the magnitude of the error, which is appropriate when the inputs are normalized, because then the ideal scale of the weights depends linearly on the magnitude of the targets.

æœ¬æ®µç€æ‰‹åˆ†ææœ€åä¸€å±‚çš„æƒé‡(<img alt="${\bf W}$" src="svgs/ce01fa56c40e0a3b42fb4a4c6c6d67fe.svg" align="middle" width="19.805940000000003pt" height="22.557149999999986pt"/>)çš„æ›´æ–°ä¸è¯¯å·®é¡¹(<img alt="$f_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (X) - Y_t$" src="svgs/6164c852e14cc14ad0919c489a7b3da4.svg" align="middle" width="108.78549pt" height="24.65759999999998pt"/>)ä¹‹é—´çš„å…³ç³», æŒ‡å‡ºæƒé‡çš„æ›´æ–°è§„æ¨¡ä¸è¯¯å·®é¡¹çš„è§„æ¨¡å‘ˆçº¿æ€§å…³ç³»ã€‚è¿›ä¸€æ­¥è¯¯å·®é¡¹æœ¬èº«çš„è§„æ¨¡åˆä¸targetçš„è§„æ¨¡å‘ˆçº¿æ€§å…³ç³»ã€‚æ­£å¸¸æƒ…å†µä¸‹(å³targetçš„è§„æ¨¡åœ¨ç›¸å¯¹å°çš„èŒƒå›´å†…æ—¶), ä»¥ä¸Šçš„æ›´æ–°æ˜¯æ²¡é—®é¢˜çš„ã€‚

> Now consider the SGD update to the parameters of <img alt="$h_ { {\boldsymbol \theta} }$" src="svgs/0c212e3dd00bcc0be7e55ad560026ac6.svg" align="middle" width="17.113965000000004pt" height="22.831379999999992pt"/>, <img alt="${\boldsymbol \theta}_ t = {\boldsymbol \theta}_ {t-1}  - \alpha {\boldsymbol J}_ t {\bf W}_ {t-1}^\intercal {\boldsymbol \delta}_ t$" src="svgs/cc69cb59ed596e20470a9b517507743a.svg" align="middle" width="174.925905pt" height="25.6047pt"/> where <img alt="${\boldsymbol J}_ t = ( \nabla h_ { {\boldsymbol \theta}, 1 }  (X), \ldots, \nabla h_ { {\boldsymbol \theta}, m } (X) )^\intercal$" src="svgs/7a491d6d3be2fa78eda0a36f9eb562ee.svg" align="middle" width="241.17835499999998pt" height="24.65759999999998pt"/> is the Jacobian for <img alt="$h_ { {\boldsymbol \theta} }$" src="svgs/0c212e3dd00bcc0be7e55ad560026ac6.svg" align="middle" width="17.113965000000004pt" height="22.831379999999992pt"/>. The magnitudes of both the weights <img alt="${\bf W}$" src="svgs/ce01fa56c40e0a3b42fb4a4c6c6d67fe.svg" align="middle" width="19.805940000000003pt" height="22.557149999999986pt"/> and the erros <img alt="${\boldsymbol \delta}$" src="svgs/ef2db80fd6c60a9bf24092a1f40a1215.svg" align="middle" width="9.212280000000005pt" height="22.831379999999992pt"/> depend linearly on the magnitude of the targets. This means that the magnitude of the update for <img alt="${\boldsymbol \theta}$" src="svgs/09ea36f6e30cbbd2fe751158784ed2e5.svg" align="middle" width="9.760245000000003pt" height="22.831379999999992pt"/> depends **quaratically** on the magnitude of the targets. *There is no compelling reason for these updates to depend at all on these magnitudes because the weights in the top layer already ensure appropriate scaling.* In other words, for each doubling of the magnitudes of the targets, the updates to the lower layers quadruple for no clear reason.

è¿›ä¸€æ­¥, lower layerçš„ç½‘ç»œå‚æ•°(<img alt="${\boldsymbol \theta}$" src="svgs/09ea36f6e30cbbd2fe751158784ed2e5.svg" align="middle" width="9.760245000000003pt" height="22.831379999999992pt"/>)æ›´æ–°å¹…åº¦æ ¹æ®å…¬å¼æ—¢ä¸<img alt="${\bf W}$" src="svgs/ce01fa56c40e0a3b42fb4a4c6c6d67fe.svg" align="middle" width="19.805940000000003pt" height="22.557149999999986pt"/>è§„æ¨¡å‘ˆçº¿æ€§å…³ç³», åˆä¸è¯¯å·®é¡¹è§„æ¨¡å‘ˆçº¿æ€§å…³ç³», ä¸¤è€…å‡ä¸targetçš„è§„æ¨¡å‘ˆçº¿æ€§å…³ç³»å¹¶ä¸”æ˜¯ç›¸ä¹˜çš„å…³ç³», ç»¼åˆå¯¼è‡´lower layerç½‘ç»œå‚æ•°æ›´æ–°å¹…åº¦ä¸targetçš„è§„æ¨¡å‘ˆå¹³æ–¹å…³ç³»ã€‚è€Œäº‹å®ä¸Šå¹¶æ²¡æœ‰æ˜¾è‘—çš„ç†ç”±ä¿è¯å¦‚æ­¤çš„å…³ç³»ã€‚ï¼ˆç›¸åè¿™å¯èƒ½é€ æˆå¯¹å‚æ•°çš„"ç ´å"ï¼‰

**æ³¨**: ä¸€èˆ¬å¤šå±‚ç½‘ç»œä¸­, lower layer | upper layerçš„æ¦‚å¿µæ˜¯æ ¹æ®æ­å»ºçš„é¡ºåºè€Œè¨€, æ¥è¿‘Inputçš„ä¸ºlower layer, è€Œæ¥è¿‘Outputçš„ä¸ºupper layerã€‚ï¼ˆæ­¤å‰å¯¹è¿™ä¸¤è€…ç†è§£æ­£å¥½ç›¸åğŸ¤£ï¼‰

> This analysis suggests an algorithmic solution, which seems to be novel in and of itself, in which we track the magnitude of the targets in a separate parameter <img alt="$\sigma_t$" src="svgs/5ba3f1b75931f41283dac26b10c8c182.svg" align="middle" width="14.358960000000003pt" height="14.155350000000013pt"/>, and then multiply the updates for all lower layers with a factor <img alt="$\sigma_t^{-2}$" src="svgs/0d565a477fd661861c27bbe250b0ae9d.svg" align="middle" width="26.809530000000006pt" height="28.89513000000001pt"/>. A more general version of this for matrix scallings is given in Algorithm 2.

æ ¹æ®ä»¥ä¸Šçš„åˆ†æ, ä½œè€…æå‡ºäº†ç›¸åº”çš„è§£å†³æ–¹æ¡ˆ, å³è·Ÿè¸ªtargetçš„è§„æ¨¡<img alt="$\sigma_t$" src="svgs/5ba3f1b75931f41283dac26b10c8c182.svg" align="middle" width="14.358960000000003pt" height="14.155350000000013pt"/>, å¹¶å¯¹lower layerçš„æ›´æ–°ç›¸åº”ä¹˜ä»¥<img alt="$\sigma_t^{-2}$" src="svgs/0d565a477fd661861c27bbe250b0ae9d.svg" align="middle" width="26.809530000000006pt" height="28.89513000000001pt"/>ä»è€Œæ¶ˆé™¤æ­¤å¤„å¼•å…¥çš„targetè§„æ¨¡çš„å½±å“ã€‚å…·ä½“çš„ç®—æ³•æµç¨‹åœ¨Algorithm 2ä¸­ç»™å‡ºï¼ˆå…¶ä¸­<img alt="${\bf W} \leftarrow {\bf W} - \alpha {\boldsymbol \delta} {\boldsymbol g}^\intercal$" src="svgs/df2b13e034551dff1ded630ad81110cd.svg" align="middle" width="122.07145499999999pt" height="22.831379999999992pt"/>ä¸­<img alt="${\boldsymbol g}$" src="svgs/3df5c9a7a58493944eea553b2055d657.svg" align="middle" width="9.566205000000004pt" height="14.61206999999998pt"/>åº”è¯¥æ˜¯<img alt="${\boldsymbol h}$" src="svgs/a0c5d722828357bd74fbe43f7f445706.svg" align="middle" width="10.974150000000005pt" height="22.831379999999992pt"/>ï¼‰ã€‚

<p align="center">
<img src="https://user-images.githubusercontent.com/16682999/64753079-747f9c00-d554-11e9-9bb3-e93407d09307.png" alt="algorithm 2" width="800">
</p>

ç®—æ³•2ä¸­çš„æ ¸å¿ƒæ­¥éª¤æ˜¯<img alt="${\boldsymbol \theta} \leftarrow {\boldsymbol \theta} - \alpha {\boldsymbol J} ( {\color{red} {\boldsymbol \Sigma} ^{-1} } {\bf W})^\intercal {\color{red} {\boldsymbol \Sigma}^{-1} } {\boldsymbol \delta}$" src="svgs/db39950acd76933ad0c313abf76e90db.svg" align="middle" width="200.461305pt" height="29.260439999999992pt"/>, å…¶ä¸­å¯¹<img alt="${\bf W}$" src="svgs/ce01fa56c40e0a3b42fb4a4c6c6d67fe.svg" align="middle" width="19.805940000000003pt" height="22.557149999999986pt"/>å’Œ<img alt="${\boldsymbol \delta}$" src="svgs/ef2db80fd6c60a9bf24092a1f40a1215.svg" align="middle" width="9.212280000000005pt" height="22.831379999999992pt"/>åˆ†åˆ«åšäº†"åŠæ ‡å‡†åŒ–"å¤„ç†, å³å°†å…¶magnitudeç½®ä¸ºä¸€ã€‚

**å°ç»“**: è‡³æ­¤, ä½œè€…æå‡ºäº†ç¬¬äºŒä¸ªç®—æ³•ã€‚è¯¥ç®—æ³•çš„æ€è·¯ä¸åŒäºç®—æ³•1å¯¹targetè¿›è¡Œæ ‡å‡†åŒ–, è€Œæ˜¯çœ‹å‡†targetè§„æ¨¡å¯¹æ¨¡å‹å‚æ•°updateçš„å½±å“ä¼šåˆ†åˆ«é€šè¿‡è¯¯å·®é¡¹(<img alt="${\boldsymbol \delta}_ t$" src="svgs/9bd6306c104510dcd2962710573f3aad.svg" align="middle" width="14.178120000000003pt" height="22.831379999999992pt"/>)å’Œtop layeræƒé‡(<img alt="${\bf W}$" src="svgs/ce01fa56c40e0a3b42fb4a4c6c6d67fe.svg" align="middle" width="19.805940000000003pt" height="22.557149999999986pt"/>)å¼•å…¥è€Œé€ æˆquadraticçš„å½±å“, è€ƒè™‘åœ¨lower layeræ¨¡å‹å‚æ•°updateä¸­æ¶ˆé™¤è¯¥å½±å“ã€‚

> We prove an interesting, and perhaps surprising, connection to the Pop-Art algorithm.

> **Proposition 3.** *Consider two functions defined by*

<p align="center"><img alt="$$&#10;f_ { {\boldsymbol \theta}, {\boldsymbol \Sigma}, {\bf W}, {\boldsymbol b} } (x) = {\boldsymbol \Sigma}({\bf W} h_ { {\boldsymbol \theta} } (x) + {\boldsymbol b} ) + {\boldsymbol \mu} \quad \text{and} \quad f_ { {\boldsymbol \theta}, {\bf W}, {\boldsymbol b} } (x) = {\bf W} h_ { {\boldsymbol \theta} } (x) + {\boldsymbol b},&#10;$$" src="svgs/659962984fe3cb7dcea2c918e94bf12b.svg" align="middle" width="495.04124999999993pt" height="17.031959999999998pt"/></p>

> *where <img alt="$h_ { {\boldsymbol \theta} }$" src="svgs/0c212e3dd00bcc0be7e55ad560026ac6.svg" align="middle" width="17.113965000000004pt" height="22.831379999999992pt"/> is the same differentiable function in both cases, and the functions are initialized identically, using <img alt="${\boldsymbol \Sigma}_ 0 = {\bf I}$" src="svgs/ccbc63de47f79763c55b599a0ba72412.svg" align="middle" width="50.113965pt" height="22.557149999999986pt"/> and <img alt="${\boldsymbol \mu}={\bf 0}$" src="svgs/a9fee4b0dbf83df610b0b48669e20e3d.svg" align="middle" width="43.00593pt" height="21.18732pt"/>, and the same initial <img alt="${\boldsymbol \theta}_ 0$" src="svgs/d89fa1c025a215ce0a649f721bd0b721.svg" align="middle" width="16.312890000000003pt" height="22.831379999999992pt"/>, <img alt="${\bf W}_ 0$" src="svgs/9f410b003037820f319071b8d2fc3f82.svg" align="middle" width="26.358420000000002pt" height="22.557149999999986pt"/> and <img alt="${\boldsymbol b}_ 0$" src="svgs/ad635100e33dc6d4795fe40a81b298fb.svg" align="middle" width="15.114165000000005pt" height="22.831379999999992pt"/>. Consider updating the first function using Algorithm 1 (Pop-Art SGD) and the second using Algorithm 2 (Normalized SDG). Then, for any sequence of non-singular scales <img alt="$\{ {\boldsymbol \Sigma}_ t \}_ {t=1}^\infty$" src="svgs/93ede31b4d2181aa0a4961a274c7bfad.svg" align="middle" width="57.488805pt" height="24.65759999999998pt"/> and shift <img alt="$\{ {\boldsymbol \mu}_ t \}_ {t=1}^\infty$" src="svgs/e82018da0d0a8a5ed808e3577c477e70.svg" align="middle" width="55.472010000000004pt" height="24.65759999999998pt"/>, the algorithms are equivalent in the sense that 1) the sequences <img alt="$\{ {\boldsymbol \theta}_ t \}_ {t=0}^\infty$" src="svgs/86209e8cb79c2f82abc3b5dad86cb30c.svg" align="middle" width="53.596125pt" height="24.65759999999998pt"/> are identical, 2) the outputs of the functions are identical, for any input.*

ä»¥ä¸Šçš„Propositioné˜è¿°äº†ç®—æ³•2å’Œç®—æ³•1çš„ç­‰æ•ˆæ€§ã€‚å½“åˆå§‹çš„ä¸Šä¸‹å±‚æƒé‡ä¸€è‡´æ—¶, å¹¶ä¸”ç®—æ³•1ä¸­åˆå§‹çš„ç»Ÿè®¡ç®—æ³•æŒ‰ç…§"å•ä½åŒ–"è®¾ç½®, åˆ™ä¸¤ä¸ªç®—æ³•ä¸‹åœ¨ä»»æ„æ—¶åˆ»çš„ç»“æœå‡æ˜¯ä¸€è‡´çš„ã€‚

> **The proposition shows a duality between normalizing the targets, as in Algorithm 1, and changing the updates, as in Algorithm 2.** This allows us to gain more intuition about the algorithm. In particular, in Algorithm 2 the updates in top layer are not normalized, thereby allowing the last linear layer to adapt to the scale of the targets. That said, these methods are complementary, and it is straightforward to combing Pop-Art with other optimization algorithms than SGD.

å†æ¬¡å¼ºè°ƒä»¥ä¸ŠPropositionçš„é‡è¦æ€§, ä¿éšœäº†ä¸¤ä¸ªç®—æ³•é—´çš„å¯¹å¶æ€§ã€‚å¦ä¸€æ–¹é¢, ç®—æ³•2ä¸­çš„top layerå¹¶æœªè¿›è¡Œæ ‡å‡†åŒ–å¤„ç†, ä»è€Œä¿ç•™äº†å…¶é€‚åº”targetçš„è°ƒæ•´ç©ºé—´ã€‚Propositionè¡¨æ˜Pop-Artç®—æ³•æœ¬èº«å¯ä»¥ä½œä¸ºå¯¹å…¶ä»–ä¼˜åŒ–ç®—æ³•çš„ä¸€ä¸ªè¡¥å……ã€‚

**å°ç»“**: æœ¬èŠ‚æå‡ºäº†é‡è¦çš„Proposition, é˜è¿°äº†ç®—æ³•2å’Œç®—æ³•1çš„ç­‰æ•ˆæ€§, å› æ­¤Pop-Artç®—æ³•å¯ä»¥ä½œä¸ºå¯¹å…¶ä»–ä¼˜åŒ–ç®—æ³•çš„ä¸€ä¸ªè¡¥å……ã€‚

### æ€»ç»“ä¸æ€è€ƒ

æœ¬æ–‡è€ƒè™‘å¼ºåŒ–å­¦ä¹ ä¸­targetè·¨åº¦å¤§è€Œå¯¼è‡´æ¨¡å‹å­¦ä¹ æ•ˆæœå·®çš„é—®é¢˜è¿›è¡Œäº†ç ”ç©¶, æå‡ºäº†Pop-Artç®—æ³•è§£å†³è¯¥é—®é¢˜ã€‚å…¶ç®—æ³•çš„æ ¸å¿ƒåŒ…æ‹¬ä¸¤ä¸ªéƒ¨åˆ†: 1) Pop (Presering outputs precisely): å³æ— è®ºæ ‡å‡†åŒ–å‚æ•°å¦‚ä½•å˜åŒ–, å·²æœ‰çš„è¾“å‡ºä¸ä¼šå˜åŒ–; 2) Art (Adaptive rescaling target): å³è‡ªé€‚åº”ç›®æ ‡å€¼æ”¾ç¼©, ä¿éšœæ–°çš„ç›®æ ‡å€¼èƒ½åˆç†çš„æ”¾ç¼©æ ‡å‡†åŒ–ã€‚æ­¤å¤–, æœ¬æ–‡è¿˜åˆ†æäº†targetè·¨åº¦å¤§å¯¼è‡´æ¨¡å‹å­¦ä¹ æ•ˆæœå·®çš„åŸå› : targetçš„è·¨åº¦å½±å“å°†"å¹³æ–¹å¼"åœ°å½±å“lower layerçš„å‚æ•°æ›´æ–°ã€‚åœ¨æ­¤åŸºç¡€ä¸Š, ä½œè€…æå‡ºäº†é’ˆå¯¹lower layeræ›´æ–°çš„ä¿®æ­£ç®—æ³•, å¹¶è¯æ˜äº†è¯¥ç®—æ³•ä¸å‰è¿°Pop-Artç®—æ³•çš„ç­‰æ•ˆæ€§ã€‚

**ç–‘é—®**: å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­å¤šä¸ºmini-batchçš„è®­ç»ƒæ¨¡å¼, é‚£ä¹ˆåœ¨éšæœºé€‰å–çš„batchä¸­å¯èƒ½å‡ºç°å¦‚ä¸‹çš„æƒ…å†µ: <img alt="$[ a_1, \ldots, a_n, b_1, \ldots, b_m]$" src="svgs/baba2abc52b0cddb5feffddf6b5af85a.svg" align="middle" width="157.168605pt" height="24.65759999999998pt"/>ã€‚å…¶ä¸­<img alt="$a$" src="svgs/44bc9d542a92714cac84e01cbbb7fd61.svg" align="middle" width="8.689230000000004pt" height="14.155350000000013pt"/>, <img alt="$b$" src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg" align="middle" width="7.054855500000005pt" height="22.831379999999992pt"/>åºåˆ—åˆ†åˆ«åœ¨ä¸åŒçš„èŒƒå›´, åŒç»„ä¸­å½¼æ­¤å·®å¼‚ä»…ä½“ç°åœ¨å°æ•°ç‚¹å3ä½, è€Œä¸¤ç»„é—´çš„å·®å¼‚ä¸ºä¸ªä½æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹, æ ‡å‡†åŒ–å¤„ç†æ˜¯å¦èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†ç»„å†…çš„å·®å¼‚å‘¢ï¼Ÿæ­¤batchçš„å¤„ç†æ˜¯å¦åº”è¯¥å°†æ ·æœ¬é€ä¸€è¾“å…¥è¿›è¡Œå¤„ç†ï¼Ÿ

**æ­¤ç–‘é—®åœ¨æ–‡ç« çš„é™„åŠ éƒ¨åˆ†ä¸­æœ‰æ‰€æ¶‰åŠ, å³éœ€è¦æ›¿æ¢"Art"éƒ¨åˆ†ã€‚**